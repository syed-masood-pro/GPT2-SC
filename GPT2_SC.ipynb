{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Spam Classification Using GPT-2 Transformer (with `tiktoken` Tokenizer)\n",
        "\n",
        "This project demonstrates the fine-tuning of a **GPT-2-based transformer** model for binary classification ‚Äî identifying whether an input SMS message is **spam** or **not spam**.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Objective\n",
        "\n",
        "Train a lightweight transformer model (GPT-2 Small - 124M) to classify text messages as:\n",
        "- üì© **Ham** ‚Äì legitimate, personal, or informative messages  \n",
        "- üö´ **Spam** ‚Äì unsolicited promotional or fraudulent content\n",
        "\n",
        "---\n",
        "\n",
        "## üìÇ Dataset\n",
        "\n",
        "The dataset is based on the classic **SMS Spam Collection**, which has been:\n",
        "- Preprocessed and cleaned\n",
        "- Balanced to ensure equal representation of `spam` and `ham` messages\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Tools & Technologies\n",
        "\n",
        "- **Model**: GPT-2 Small (124M parameters)\n",
        "- **Tokenizer**: [`tiktoken`](https://github.com/openai/tiktoken) ‚Äî optimized BPE tokenizer from OpenAI  \n",
        "- **Framework**: PyTorch\n",
        "- **Training & Evaluation**: Custom loaders, loss/accuracy evaluation, partial layer unfreezing\n",
        "- **Visualization**: Matplotlib for plotting training curves\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ Workflow Summary\n",
        "\n",
        "1. **Preprocessing**\n",
        "   - Label encoding: `\"spam\" ‚Üí 1`, `\"ham\" ‚Üí 0`\n",
        "   - Tokenization using `tiktoken`, padding/truncation to fit GPT context (‚â§1024 tokens)\n",
        "\n",
        "2. **Model Customization**\n",
        "   - Load GPT-2 architecture from scratch\n",
        "   - Freeze all but the final transformer block and output head\n",
        "   - Replace the original `out_head` to classify into 2 classes\n",
        "\n",
        "3. **Training**\n",
        "   - Fine-tune with CrossEntropyLoss\n",
        "   - Evaluate periodically on a validation split\n",
        "   - Use `AdamW` optimizer with weight decay\n",
        "\n",
        "4. **Evaluation**\n",
        "   - Compute accuracy and loss on:\n",
        "     - Training set\n",
        "     - Validation set\n",
        "     - Test set\n",
        "\n",
        "5. **Visualization**\n",
        "   - Line plots for training/validation loss and accuracy\n",
        "\n",
        "6. **Inference**\n",
        "   - Classify custom messages using the fine-tuned model\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Performance Snapshot\n",
        "\n",
        "| Dataset    | Accuracy (%) | Loss   |\n",
        "|------------|--------------|--------|\n",
        "| Training   | 98.75%       | 0.772  |\n",
        "| Validation | 95.97%       | 0.764  |\n",
        "| Test       | 94.00%       | 0.789  |\n",
        "\n",
        "> ‚ö†Ô∏è The results indicate strong performance, with the model generalizing well even with limited fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Sample Predictions\n",
        "\n",
        "```python\n",
        "text = \"Congratulations! You've been selected for a free cruise to the Bahamas.\"\n",
        "print(classify_review(text, model, tokenizer, device))  # ‚ûú spam\n",
        "\n",
        "text = \"Hey, are we still on for the meeting at 3 PM?\"\n",
        "print(classify_review(text, model, tokenizer, device))  # ‚ûú not spam\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Conclusion\n",
        "\n",
        "By leveraging the capabilities of GPT-2 and the efficiency of `tiktoken`, we achieved high accuracy on a binary classification task using minimal labeled data and only partial model fine-tuning. This approach can be adapted to similar tasks like phishing detection, toxic comment filtering, or support ticket triage.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "uuK01V51Niu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "caYPW731ZG4M"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "8W-15V-Hrdw3"
      },
      "outputs": [],
      "source": [
        "# Model configuration dictionary (124M parameter version)\n",
        "GPT_CONFIG_124m={\n",
        "    'vocab_size':50257,        # Size of the vocabulary; used in the embedding layer\n",
        "    'context_length':256,     # Maximum number of tokens the model can consider at once (sequence length)\n",
        "    'emb_dim':768,             # Dimensionality of the token embeddings and hidden states\n",
        "    'n_heads':12,              # Number of attention heads in the multi-head attention mechanism\n",
        "    'n_layers':12,             # Number of transformer blocks/layers in the model\n",
        "    'dropout':0.1,             # Dropout rate used during training to prevent overfitting\n",
        "    'qkv_bias':False           # Whether to use bias terms in the query, key, and value projection layers\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPLEMENTING MULTI-HEAD ATTENTION WITH WEIGHT SPLITS"
      ],
      "metadata": {
        "id": "WGPqSjbOh3fD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This implementation of `MultiHeadAttention` efficiently computes multi-head self-attention by splitting and processing attention heads in parallel within a single module.\n",
        "\n",
        "---\n",
        "\n",
        "#### Key Highlights:\n",
        "\n",
        "- **Query, Key, Value Projections**: The input tensor is projected using three separate linear layers to obtain queries, keys, and values.\n",
        "- **Head Splitting with Reshape and Transpose**: The output dimension (`d_out`) is divided into `num_heads` and `head_dim`, enabling parallel attention processing across multiple heads.\n",
        "- **Causal Masking**: A causal mask is applied to ensure that each position can only attend to previous and current positions in the sequence.\n",
        "- **Output Projection**: After computing attention for each head and combining the results, a final linear layer projects the output back to the original embedding size.\n",
        "\n",
        "---\n",
        "\n",
        "#### Step-by-Step Breakdown:\n",
        "\n",
        "1. **Linear Projections**: The input is linearly transformed into queries, keys, and values of shape `(b, num_tokens, d_out)`.\n",
        "2. **Head Reshaping**: These are reshaped to `(b, num_tokens, num_heads, head_dim)` using `.view()`.\n",
        "3. **Transposition**: The tensors are transposed to `(b, num_heads, num_tokens, head_dim)` to align for batched matrix multiplication.\n",
        "4. **Attention Scores**: Attention scores are computed using the scaled dot product: `queries @ keys.T`, scaled by `sqrt(head_dim)`.\n",
        "5. **Masking**: A causal mask is applied to the upper triangular part to block future tokens.\n",
        "6. **Softmax and Dropout**: The masked attention scores are passed through softmax and dropout for regularization.\n",
        "7. **Context Vector**: Attention weights are used to compute a weighted sum of values, producing the context vector.\n",
        "8. **Combining Heads**: The heads are transposed back and reshaped into a single tensor of shape `(b, num_tokens, d_out)`.\n",
        "9. **Output Projection**: The final context vector is passed through an output linear layer (`self.out_proj`).\n",
        "\n",
        "---\n",
        "\n",
        "This structure enables efficient multi-head attention using batched operations and avoids redundant computation, making it well-suited for large-scale Transformer models.\n"
      ],
      "metadata": {
        "id": "RS6G3u2IizJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing scaled dot-product attention with masking\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False):\n",
        "    # Just making sure the output size works with how many heads we want\n",
        "    # Ensure the output dimension is divisible by the number of heads\n",
        "    super().__init__()\n",
        "    assert (d_out%num_heads==0),'d_out must be divisible by num_heads'\n",
        "\n",
        "    self.d_out=d_out\n",
        "    self.num_heads=num_heads\n",
        "    self.head_dim=d_out//num_heads              # Each head gets a slice of the total embedding\n",
        "\n",
        "    # Linear layers to get Q, K, V\n",
        "    self.W_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.out_proj=nn.Linear(d_out,d_out)        # this layer use to combine head outputs\n",
        "    self.dropout=nn.Dropout(dropout)            # Dropout layer to prevent overfitting\n",
        "    # Causal mask to prevent attention to future tokens\n",
        "    self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,num_tokens,d_in=x.shape                  # b: batch size, num_tokens: sequence length\n",
        "    # Linear projections\n",
        "    queries=self.W_query(x)\n",
        "    keys=self.W_key(x)                         # Shape: (b, num_tokens, d_out)\n",
        "    values=self.W_value(x)\n",
        "\n",
        "    # Break each into multiple heads for parallel attention\n",
        "    #implicitly split the matrix by adding a `num_heads` dimension\n",
        "    # Unroll last dim: (b, num_tokens, d_out) to (b, num_tokens, num_heads, head_dim)\n",
        "    keys=keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    queries=queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    values=values.view(b,num_tokens,self.num_heads,self.head_dim) # Changed self.head to self.head_dim\n",
        "\n",
        "    #transpose: (b,num_tokens,num_heads,head_dim) to (b,num_heads,num_tokens,head_dim)\n",
        "    keys=keys.transpose(1,2)\n",
        "    queries=queries.transpose(1,2)\n",
        "    values=values.transpose(1,2)\n",
        "\n",
        "    #Dot prod of each head\n",
        "    # Compute scaled dot-product attention(self-attention) with a causal mask\n",
        "    attn_scores=queries @ keys.transpose(2,3)       # Shape: (b, heads, seq_len, seq_len)\n",
        "\n",
        "    # Original mask truncated to the number of tokens and converted to boolean\n",
        "    #####mask_bool=self.mask.bool()[:num_tokens,:num_tokens]\n",
        "    mask_bool = self.mask[:num_tokens, :num_tokens].bool().to(x.device)\n",
        "    # Use the mask to fill attention scores\n",
        "    attn_scores.masked_fill_(mask_bool,-torch.inf)\n",
        "    ######attn_scores.masked_fill_(mask_bool, -1e9)\n",
        "\n",
        "    attn_weights=torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n",
        "    attn_weights=self.dropout(attn_weights)\n",
        "\n",
        "    # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "    context_vec=(attn_weights @values).transpose(1,2)\n",
        "\n",
        "    # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "    context_vec=context_vec.contiguous().view(b,num_tokens,self.d_out) # contiguous is used for reshape matrices in same block of memory\n",
        "    context_vec=self.out_proj(context_vec)\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "tf4pVq7Vx93j"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Layer Normalization with Learnable Parameters\n",
        "\n",
        "This is a custom implementation of Layer Normalization, which normalizes the input across the last dimension (usually the embedding dimension) and includes two trainable parameters: **scale (Œ≥)** and **shift (Œ≤)**.\n",
        "\n",
        "---\n",
        "\n",
        "#### Key Components:\n",
        "\n",
        "- **Mean and Variance Computation**:\n",
        "  - Mean and variance are computed across the last dimension for each input tensor.\n",
        "  - `unbiased=False` is used to divide by `n` instead of `n-1` (Bessel‚Äôs correction is avoided for efficiency).\n",
        "\n",
        "- **Normalization Formula**:\n",
        "  $$\n",
        "  \\text{norm}_x = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
        "  $$\n",
        "\n",
        "- **Learnable Parameters**:\n",
        "  - `scale (Œ≥)`: Initialized as ones, used to scale the normalized output.\n",
        "  - `shift (Œ≤)`: Initialized as zeros, used to shift the normalized output.\n",
        "\n",
        "- **Epsilon (Œµ)**:\n",
        "  - A small constant (`1e-5`) added to the variance to prevent division by zero.\n",
        "\n",
        "---\n",
        "\n",
        "#### Forward Pass Steps:\n",
        "\n",
        "1. Compute the **mean** and **variance** along the last dimension.\n",
        "2. Normalize the input: subtract the mean and divide by the standard deviation.\n",
        "3. Apply learnable **scale** and **shift** parameters.\n",
        "4. Return the final normalized and parameter-adjusted output.\n",
        "\n",
        "---\n",
        "\n",
        "This layer helps stabilize training by ensuring consistent input distributions throughout the network, especially beneficial in deep Transformer architectures.\n"
      ],
      "metadata": {
        "id": "uj5TzThFjLfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing Custom layer normalization with learnable parameters\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps=1e-5\n",
        "    self.scale=nn.Parameter(torch.ones(emb_dim))          # Learnable scale parameter (Œ≥)\n",
        "    self.shift=nn.Parameter(torch.zeros(emb_dim))         # Learnable shift parameter (Œ≤)\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean=x.mean(dim=-1,keepdim=True)                      # Compute mean along the last dimension\n",
        "    var=x.var(dim=-1,keepdim=True,unbiased=False)         # unbiased is false bcz of to divide by n instead of n-1(bessels correction)\n",
        "    norm_x=(x-mean)/torch.sqrt(var+self.eps)              # Normalize input\n",
        "    return self.scale* norm_x + self.shift                # Apply learnable scale and shift"
      ],
      "metadata": {
        "id": "1Yw0sP3-sV5D"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Implementation of the GELU Activation Function\n",
        "\n",
        "This is a custom PyTorch implementation of the **Gaussian Error Linear Unit (GELU)** activation, commonly used in Transformer models such as BERT and GPT.\n",
        "\n",
        "---\n",
        "\n",
        "#### What is GELU?\n",
        "\n",
        "GELU is a smooth and differentiable activation function that combines properties of both ReLU and sigmoid functions. Instead of abruptly cutting off values like ReLU, GELU weights inputs based on their magnitude.\n",
        "\n",
        "---\n",
        "\n",
        "#### Approximation Formula:\n",
        "\n",
        "The original GELU function is defined using the Gaussian cumulative distribution function. However, for efficiency, a commonly used tanh-based approximation is:\n",
        "\n",
        "$$\n",
        "\\text{GELU}(x) = 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\cdot \\left(x + 0.044715 \\cdot x^3\\right)\\right]\\right)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### Forward Pass:\n",
        "\n",
        "1. Compute the cubic term: \\( 0.044715 \\cdot x^3 \\)\n",
        "2. Add it to the input: \\( x + 0.044715 \\cdot x^3 \\)\n",
        "3. Multiply by \\( \\sqrt{2/\\pi} \\) and apply `tanh`\n",
        "4. Multiply the result by \\( 0.5 \\cdot x \\)\n",
        "\n",
        "This provides a smooth, non-linear activation that performs well in practice, especially in NLP models.\n",
        "\n",
        "---\n",
        "\n",
        "#### Why Use GELU?\n",
        "\n",
        "- It allows small negative values to pass (unlike ReLU)\n",
        "- Leads to better gradient flow\n",
        "- Used in state-of-the-art transformer-based architectures\n"
      ],
      "metadata": {
        "id": "M7403d_2jk_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom implementation of the Gaussian Error Linear Unit (GELU) activation function\n",
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2/torch.pi))*(x+0.044715*torch.pow(x,3))))     # Apply the GELU activation using the tanh-based approximation"
      ],
      "metadata": {
        "id": "O2lAPyl-tlq-"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed-Forward Network in Transformer Block\n",
        "\n",
        "This module implements the **position-wise feed-forward network** used inside Transformer blocks. It is a crucial component that applies non-linearity and learns richer representations after the attention mechanism.\n",
        "\n",
        "---\n",
        "\n",
        "#### Structure:\n",
        "\n",
        "The feed-forward network is composed of two linear transformations with a non-linear activation (GELU) in between:\n",
        "\n",
        "1. **Expansion Layer**:\n",
        "   - Projects the input embedding from size `emb_dim` to `4 * emb_dim`\n",
        "   - This widening helps the model capture more complex interactions\n",
        "2. **GELU Activation**:\n",
        "   - Adds smooth non-linearity\n",
        "3. **Projection Layer**:\n",
        "   - Reduces the dimensionality back to `emb_dim` to match the input shape\n",
        "\n",
        "---\n",
        "\n",
        "#### Implemented as:\n",
        "\n",
        "```python\n",
        "nn.Sequential(\n",
        "    nn.Linear(emb_dim, 4 * emb_dim),  # Expand\n",
        "    GELU(),                            # Apply GELU\n",
        "    nn.Linear(4 * emb_dim, emb_dim)    # Project back\n",
        ")\n"
      ],
      "metadata": {
        "id": "OSEgZjfRj1E2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of the position-wise feed-forward network used in Transformer blocks\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    # Defining a two-layer feed-forward network with GELU activation\n",
        "    #The hidden layer expands the embedding dimension by a factor of 4\n",
        "    self.layers=nn.Sequential(\n",
        "        nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),       # First linear layer: expansion\n",
        "        GELU(),\n",
        "        nn.Linear(4*cfg['emb_dim'],cfg['emb_dim'])        # Second linear layer: projection back\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)                                 # Pass input through the feed-forward layers"
      ],
      "metadata": {
        "id": "MBM5nyIOwzDm"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Block with Residual Connections and Layer Normalization\n",
        "\n",
        "This module defines a single **Transformer block**, one of the key building units in architectures like GPT and BERT. It consists of two main components:\n",
        "\n",
        "1. **Multi-Head Self-Attention**\n",
        "2. **Position-Wise Feed-Forward Network**\n",
        "\n",
        "Each sublayer is wrapped with **Layer Normalization** and a **Residual Connection** to enhance gradient flow and model stability during training.\n",
        "\n",
        "---\n",
        "\n",
        "#### Architecture Overview:\n",
        "\n",
        "The block follows the **Pre-Norm** layout, where Layer Normalization is applied **before** each sublayer:\n",
        "\n",
        "1. **Sublayer 1 (Self-Attention Path)**:\n",
        "   - Input ‚Üí LayerNorm ‚Üí Multi-Head Attention ‚Üí Dropout ‚Üí Add Residual\n",
        "\n",
        "2. **Sublayer 2 (Feed-Forward Path)**:\n",
        "   - Output of Sublayer 1 ‚Üí LayerNorm ‚Üí FeedForward ‚Üí Dropout ‚Üí Add Residual\n",
        "\n",
        "---\n",
        "\n",
        "#### Detailed Steps:\n",
        "\n",
        "- `LayerNorm (norm1)`:\n",
        "  - Normalizes input before self-attention.\n",
        "- `MultiHeadAttention`:\n",
        "  - Applies attention across multiple heads with masking.\n",
        "- `Dropout`:\n",
        "  - Prevents overfitting and regularizes residual paths.\n",
        "- `Residual Connection`:\n",
        "  - Adds the original input back to the output (i.e., `x + shortcut`).\n",
        "- `LayerNorm (norm2)`:\n",
        "  - Normalizes output of attention before feed-forward layer.\n",
        "- `FeedForward`:\n",
        "  - Two-layer network with GELU activation that transforms token representations.\n",
        "- Final output is the sum of the feed-forward output and its residual.\n",
        "\n",
        "---\n",
        "\n",
        "#### Why Residual + Normalization?\n",
        "\n",
        "- **Residual Connections**: Help preserve the original signal and gradients during backpropagation.\n",
        "- **Layer Normalization**: Stabilizes training and ensures consistent data distribution across layers.\n",
        "- **Dropout**: Adds noise during training to improve generalization.\n",
        "\n",
        "---\n",
        "\n",
        "This modular design makes the Transformer block both deep and expressive while maintaining training stability.\n"
      ],
      "metadata": {
        "id": "vHXaeFtckENr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of a single Transformer block with residual connections and normalization\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    # Multi-head self-attention layer\n",
        "    self.att=MultiHeadAttention(\n",
        "        d_in=cfg['emb_dim'],\n",
        "        d_out=cfg['emb_dim'],\n",
        "        context_length=cfg['context_length'],\n",
        "        num_heads=cfg['n_heads'],\n",
        "        dropout=cfg['dropout'],\n",
        "        qkv_bias=cfg['qkv_bias']\n",
        "    )\n",
        "\n",
        "    # Position-wise feed-forward layer\n",
        "    self.ff=FeedForward(cfg)\n",
        "\n",
        "    # Layer normalization before attention and feed-forward sublayers\n",
        "    self.norm1=LayerNorm(cfg['emb_dim'])\n",
        "    self.norm2=LayerNorm(cfg['emb_dim'])\n",
        "\n",
        "    # Dropout applied to residual connections\n",
        "    self.drop_shortcut=nn.Dropout(cfg['dropout'])\n",
        "\n",
        "  def forward(self,x):\n",
        "    # First sublayer: LayerNorm ‚Üí Attention ‚Üí Dropout ‚Üí Residual Add\n",
        "    shortcut=x\n",
        "    x=self.norm1(x)\n",
        "    x=self.att(x)\n",
        "    x=self.drop_shortcut(x)\n",
        "    x=x+shortcut              # Residual connection\n",
        "\n",
        "    # Second sublayer: LayerNorm ‚Üí FeedForward ‚Üí Dropout ‚Üí Residual Add\n",
        "    shortcut=x\n",
        "    x=self.norm2(x)\n",
        "    x=self.ff(x)\n",
        "    x=self.drop_shortcut(x)\n",
        "    x=x+shortcut              # Residual connection\n",
        "    return x"
      ],
      "metadata": {
        "id": "1pMYOFWuxbEq"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full GPT-Style Transformer Model\n",
        "\n",
        "This module defines a complete GPT-style autoregressive Transformer model designed for next-token prediction. It follows the architectural principles of GPT, including:\n",
        "\n",
        "- **Token and Positional Embeddings**\n",
        "- **Stacked Transformer Blocks**\n",
        "- **Layer Normalization**\n",
        "- **Final Projection to Vocabulary**\n",
        "\n",
        "---\n",
        "\n",
        "#### Model Components:\n",
        "\n",
        "- **Token Embedding (`tok_emb`)**:\n",
        "  - Converts input token indices into dense vectors of size `emb_dim`.\n",
        "\n",
        "- **Positional Embedding (`pos_emb`)**:\n",
        "  - Learns a unique embedding for each position in the sequence to encode order information.\n",
        "\n",
        "- **Embedding Dropout (`drop_emb`)**:\n",
        "  - Regularizes the input embeddings by randomly dropping units during training.\n",
        "\n",
        "- **Transformer Stack (`trf_block`)**:\n",
        "  - A stack of `n_layers` Transformer blocks, each containing multi-head self-attention and a feed-forward network.\n",
        "\n",
        "- **Final Layer Normalization (`final_norm`)**:\n",
        "  - Applied after all Transformer blocks to stabilize output representations.\n",
        "\n",
        "- **Output Head (`out_head`)**:\n",
        "  - A linear layer that projects the final hidden states to the vocabulary size for next-token prediction.\n",
        "\n",
        "---\n",
        "\n",
        "#### Forward Pass Flow:\n",
        "\n",
        "1. **Input Shape**: `(batch_size, sequence_length)`\n",
        "2. Compute:\n",
        "   - Token embeddings ‚Üí `(batch_size, seq_len, emb_dim)`\n",
        "   - Positional embeddings ‚Üí `(seq_len, emb_dim)`\n",
        "3. **Add Embeddings**:\n",
        "   - Combine token and position embeddings element-wise.\n",
        "4. **Apply Dropout**:\n",
        "   - Regularize combined embeddings before passing to the Transformer.\n",
        "5. **Pass Through Transformer Blocks**:\n",
        "   - Apply deep self-attention and feed-forward processing.\n",
        "6. **Final Normalization**:\n",
        "   - Normalize outputs from the last Transformer block.\n",
        "7. **Output Projection**:\n",
        "   - Generate logits for each token position over the vocabulary ‚Üí shape: `(batch_size, seq_len, vocab_size)`\n",
        "\n",
        "---\n",
        "\n",
        "#### Output:\n",
        "\n",
        "- The final `logits` tensor is typically passed to a cross-entropy loss for training language models, where the model learns to predict the next token given all previous tokens in the sequence.\n",
        "\n",
        "This design mirrors the structure of models like GPT-2 and GPT-3, enabling efficient and powerful language modeling.\n"
      ],
      "metadata": {
        "id": "m_u9BsPQkRlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full GPT-style Transformer model implementation\n",
        "class GPTModel(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb=nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])             # Token embedding layer: maps token indices to embedding vectors\n",
        "    self.pos_emb=nn.Embedding(cfg['context_length'],cfg['emb_dim'])         # Positional embedding layer: learns position information for each token\n",
        "    self.drop_emb=nn.Dropout(cfg['dropout'])                                # Dropout applied to embeddings to regularize training\n",
        "\n",
        "    # Stack of Transformer blocks\n",
        "    self.trf_block=nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
        "\n",
        "    self.final_norm=LayerNorm(cfg['emb_dim'])                               # Final layer normalization applied after all Transformer blocks\n",
        "    self.out_head=nn.Linear(cfg['emb_dim'],cfg['vocab_size'],bias=False)    # Output projection to vocabulary size (used for next-token prediction)\n",
        "\n",
        "  def forward(self,in_idx):\n",
        "    batch_size,seq_len=in_idx.shape                   # Input: in_idx of shape (batch_size, sequence_length)\n",
        "\n",
        "    # Compute token and position embeddings\n",
        "    tok_embeds=self.tok_emb(in_idx)                   # (batch_size, seq_len, emb_dim)\n",
        "    pos_embeds=self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n",
        "\n",
        "    # Combine token and positional embeddings\n",
        "    x=tok_embeds+pos_embeds               # shape: batch,num_tokens,emb_size\n",
        "\n",
        "    x=self.drop_emb(x)                    # Apply dropout to combined embeddings\n",
        "    x=self.trf_block(x)                   # Pass through stacked Transformer blocks\n",
        "    x=self.final_norm(x)                  # Apply final normalization\n",
        "    logits=self.out_head(x)               # Compute logits over vocabulary (batch_size, seq_len, vocab_size)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "ZipoM1R49UKu"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Total Parameters in the GPT Model\n",
        "\n",
        "To assess the size and capacity of the model, we compute the **total number of trainable parameters** using:\n",
        "\n"
      ],
      "metadata": {
        "id": "DeLzd9BmkdVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params=sum(p.numel() for p in GPTModel(GPT_CONFIG_124m).parameters())\n",
        "print(f'Total number of parameters: {total_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSlpd7HpaXCI",
        "outputId": "23e17a23-b5bb-424d-ffce-00764055c03c"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 162419712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating GPT Parameters (Excluding Output Head)\n",
        "\n",
        "To better understand the contribution of different parts of the model, we can compute the total number of parameters **excluding the final output projection layer (`out_head`)**:\n"
      ],
      "metadata": {
        "id": "OZvIGDm0kli2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_param_GPT= total_params- sum(p.numel() for p in GPTModel(GPT_CONFIG_124m).out_head.parameters())\n",
        "print(f'Total number of parameters: {total_param_GPT}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sE_V-kEhzpJ",
        "outputId": "d439f2c8-a267-4712-9d92-515f1103a000"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 123822336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy Decoding for Autoregressive Text Generation\n",
        "\n",
        "This function implements **greedy decoding** for generating text using a GPT-style autoregressive model. It predicts tokens one by one, always selecting the most probable next token at each step.\n",
        "\n",
        "---\n",
        "\n",
        "#### Function: `generate_text_simple(model, idx, max_new_tokens, context_size)`\n",
        "\n",
        "- **`model`**: The trained GPT model.\n",
        "- **`idx`**: Tensor containing the initial token sequence (shape: `(batch_size, seq_len)`).\n",
        "- **`max_new_tokens`**: Total number of new tokens to generate.\n",
        "- **`context_size`**: Maximum number of tokens from the past to consider for each prediction (sliding window).\n",
        "\n",
        "---\n",
        "\n",
        "#### Decoding Loop:\n",
        "\n",
        "For each step (token generation):\n",
        "\n",
        "1. **Context Truncation**:\n",
        "   - Extract only the last `context_size` tokens from `idx` to form `idx_cond`.\n",
        "   - This simulates the model‚Äôs fixed-length attention window.\n",
        "\n",
        "2. **Forward Pass** (no gradients):\n",
        "   - Pass `idx_cond` through the model to get `logits` of shape `(batch, seq_len, vocab_size)`.\n",
        "\n",
        "3. **Logit Selection**:\n",
        "   - Extract the last token's logits: `logits[:, -1, :]` ‚Üí `(batch, vocab_size)`\n",
        "\n",
        "4. **Softmax and Greedy Selection**:\n",
        "   - Compute token probabilities using softmax.\n",
        "   - Use `argmax` to select the most likely next token.\n",
        "\n",
        "5. **Append Prediction**:\n",
        "   - Concatenate the predicted token to the input `idx` for the next iteration.\n",
        "\n",
        "---\n",
        "\n",
        "#### Notes:\n",
        "\n",
        "- **Greedy decoding** always picks the most probable next token, which makes it fast but can lead to repetitive or less diverse outputs.\n",
        "- Unlike sampling-based approaches, this method **lacks randomness** and exploration.\n",
        "\n",
        "---\n",
        "\n",
        "This function is simple and deterministic, making it ideal for testing how well the model performs in generating predictable sequences.\n"
      ],
      "metadata": {
        "id": "_df6HTKIktsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedy decoding method for autoregressive text generation\n",
        "def generate_text_simple(model,idx,max_new_tokens,context_size):\n",
        "  for _ in range(max_new_tokens):\n",
        "    # to extract last context_size idx only\n",
        "    idx_cond=idx[:,-context_size:]\n",
        "\n",
        "     # Perform forward pass without tracking gradients\n",
        "    with torch.no_grad():\n",
        "      logits=model(idx_cond)                                #shape: (batch,num_tokens,vocab_size)\n",
        "\n",
        "    #extract last row from logit tensor\n",
        "    logits=logits[:,-1,:]                                   #shape: (batch,num_tokens,vocab_size) to (batch,vocab_size)\n",
        "\n",
        "    # Compute token probabilities using softmax\n",
        "    probas=torch.softmax(logits,dim=-1)\n",
        "    idx_next=torch.argmax(probas,dim=-1,keepdim=True)       # Select the most probable next token (greedy choice)  Shape: (batch_size, 1)\n",
        "    idx=torch.cat((idx,idx_next),dim=1)                     # Concatenate the predicted token to the input sequence (batch,num_token+1)\n",
        "\n",
        "  return idx"
      ],
      "metadata": {
        "id": "AUGhsqOGh5t4"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q tiktoken"
      ],
      "metadata": {
        "id": "I_rs-kwfkBS-"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer=tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "ma5U_uAcjmpU"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_context='Hello, I am'\n",
        "encoded =tokenizer.encode(start_context)\n",
        "encoded_tensor=torch.tensor(encoded).unsqueeze(0)"
      ],
      "metadata": {
        "id": "Q5885j5yjcNv"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=GPTModel(GPT_CONFIG_124m)"
      ],
      "metadata": {
        "id": "rZZui1SdkTUe"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "out=generate_text_simple(model=model,idx=encoded_tensor,max_new_tokens=6,context_size=GPT_CONFIG_124m['context_length'])\n",
        "decode_text=tokenizer.decode((out.squeeze(0).tolist()))\n",
        "decode_text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "46aglZH4kG2G",
        "outputId": "94f477d7-d0a2-48df-dcf0-a3344bb98b89"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, I am ostr ingenuityubes smell Education154'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/the-verdict.txt','r',encoding='utf-8') as f:\n",
        "  text_data=f.read()"
      ],
      "metadata": {
        "id": "hlNkD4Xrm6r7"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_char=len(text_data)\n",
        "total_token=len(tokenizer.encode(text_data))\n",
        "print(f'Total characters: {total_char}')\n",
        "print(f'Total tokens: {total_token}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9IQoOhUn4Q3",
        "outputId": "5e334d3e-3cde-44dd-9973-1b43789b2702"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 20479\n",
            "Total tokens: 5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### GPT Dataset for Autoregressive Language Modeling (Sliding Window)\n",
        "\n",
        "This section defines a custom PyTorch `Dataset` and a helper function to create a `DataLoader` for training GPT-style models. It prepares overlapping input-target token sequences using a sliding window approach.\n",
        "\n",
        "---\n",
        "\n",
        "#### `GPT_Dataset_v1` Class\n",
        "\n",
        "A dataset class for autoregressive next-token prediction.\n",
        "\n",
        "**Key Parameters:**\n",
        "- `txt`: The full input text.\n",
        "- `tokenizer`: A tokenizer (here, GPT-2 from `tiktoken`) that converts text into token IDs.\n",
        "- `max_length`: The number of tokens in each input sequence.\n",
        "- `stride`: The step size for the sliding window, controlling the amount of overlap between sequences.\n",
        "\n",
        "---\n",
        "\n",
        "#### Dataset Logic:\n",
        "\n",
        "1. **Tokenization**:\n",
        "   - The entire input text is tokenized once using the GPT-2 tokenizer, including special tokens like `<|endoftext|>`.\n",
        "\n",
        "2. **Sliding Window Sequence Creation**:\n",
        "   - For every step of `stride`, create:\n",
        "     - `input_chunk`: Sequence of `max_length` tokens.\n",
        "     - `target_chunk`: Same as input but shifted one position to the right.\n",
        "\n",
        "3. **Input-Target Pair**:\n",
        "   - Each input sequence is paired with a target sequence for predicting the next token at each position.\n",
        "\n",
        "4. **Return Format**:\n",
        "   - `__getitem__`: Returns a tuple `(input_ids, target_ids)` for training.\n",
        "\n",
        "---\n",
        "\n",
        "#### `create_dataloader_v1()` Function\n",
        "\n",
        "This helper function initializes the dataset and returns a PyTorch `DataLoader`.\n",
        "\n",
        "**Key Parameters:**\n",
        "- `txt`: Full training corpus.\n",
        "- `batch_size`: Number of sequences per batch.\n",
        "- `max_length`: Length of each token sequence.\n",
        "- `stride`: Overlap between consecutive sequences.\n",
        "- `shuffle`: Whether to shuffle the data at each epoch.\n",
        "- `drop_last`: Drop last batch if it's smaller than `batch_size`.\n",
        "- `num_workers`: Number of worker threads for loading data.\n",
        "\n",
        "---\n",
        "\n",
        "#### Why Use a Sliding Window?\n",
        "\n",
        "- Allows efficient utilization of long texts by generating multiple training samples.\n",
        "- Introduces overlap, improving model context and reducing training data sparsity.\n",
        "\n",
        "This approach is commonly used for **language modeling** tasks where the model learns to predict the next token given a context window.\n"
      ],
      "metadata": {
        "id": "ePoFsK_Hk7yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "# Dataset for autoregressive language modeling using a sliding window approach\n",
        "class GPT_Dataset_v1(Dataset):\n",
        "  def __init__(self,txt,tokenizer,max_length,stride):\n",
        "    self.input_ids=[]\n",
        "    self.target_ids=[]\n",
        "    #tokenize entire text\n",
        "    token_ids=tokenizer.encode(txt,allowed_special={'<|endoftext|>'})\n",
        "\n",
        "    #using sliciding window to chunk the txt into overlapping sequence of max_length\n",
        "    for i in range(0,len(token_ids)-max_length,stride):\n",
        "      input_chunk=token_ids[i : i+max_length]\n",
        "      target_chunk=token_ids[i+1 : i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)        # Return the number of sequence pairs\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.input_ids[index],self.target_ids[index]     # Return input-target pair at the specified index\n",
        "\n",
        "#Creating DataLoader for LM training\n",
        "def create_dataloader_v1(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
        "  tokenizer=tiktoken.get_encoding('gpt2')                  # Load GPT-2 tokenizer\n",
        "  dataset=GPT_Dataset_v1(txt,tokenizer,max_length,stride)\n",
        "  # Create the DataLoader for batching and shuffling\n",
        "  dataloader=DataLoader(\n",
        "      dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      drop_last=drop_last,\n",
        "      num_workers=num_workers\n",
        "  )\n",
        "  return dataloader\n"
      ],
      "metadata": {
        "id": "zH4WqQj7oEC9"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio=0.90\n",
        "split_idx=int(train_ratio * len(text_data))\n",
        "train_data=text_data[:split_idx]\n",
        "val_data=text_data[split_idx:]"
      ],
      "metadata": {
        "id": "bxfoSob1r7Kv"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124m['context_length'],\n",
        "    stride=GPT_CONFIG_124m['context_length'],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader=create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124m['context_length'],\n",
        "    stride=GPT_CONFIG_124m['context_length'],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "Pp5Uhhc5tWId"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"val_loader length:\", len(val_loader))  # Should be > 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luzRAgjer8QJ",
        "outputId": "fde7dd85-0b78-4ff7-deff-73ab7d63c1a6"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_loader length: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy Loss Computation for Language Modeling\n",
        "\n",
        "This section defines utility functions to compute the **cross-entropy loss** for autoregressive language modeling using:\n",
        "\n",
        "1. `calc_loss_batch()` ‚Äî Computes the loss for a **single batch**\n",
        "2. `calc_loss_loader()` ‚Äî Computes the average loss across **multiple batches** in a `DataLoader`\n",
        "\n",
        "---\n",
        "\n",
        "#### `calc_loss_batch(input_batch, target_batch, model, device)`\n",
        "\n",
        "This function computes the **cross-entropy loss** between the model's predicted logits and the true next-token targets for a single batch.\n",
        "\n",
        "**Key Steps:**\n",
        "\n",
        "- Move inputs and targets to the specified device (CPU/GPU).\n",
        "- Perform a forward pass to get `logits` of shape `(batch_size, seq_len, vocab_size)`.\n",
        "- Flatten the logits and targets:\n",
        "  - `logits.flatten(0, 1)` ‚Üí shape: `(batch_size * seq_len, vocab_size)`\n",
        "  - `target_batch.flatten()` ‚Üí shape: `(batch_size * seq_len)`\n",
        "- Apply `F.cross_entropy()` to compute token-wise classification loss.\n",
        "\n",
        "---\n",
        "\n",
        "#### `calc_loss_loader(data_loader, model, device, num_batches=None)`\n",
        "\n",
        "Computes the **average cross-entropy loss** over a number of batches drawn from a `DataLoader`.\n",
        "\n",
        "**Key Logic:**\n",
        "\n",
        "- If `num_batches` is not specified, evaluate on the entire dataset.\n",
        "- Otherwise, limit to `min(num_batches, len(data_loader))`.\n",
        "- For each batch:\n",
        "  - Use `calc_loss_batch()` to compute the loss.\n",
        "  - Accumulate total loss.\n",
        "- Return the **mean loss** across the evaluated batches.\n",
        "\n",
        "---\n",
        "\n",
        "#### Notes:\n",
        "\n",
        "- This setup is ideal for:\n",
        "  - Evaluating training or validation loss.\n",
        "  - Monitoring model performance during training.\n",
        "- The cross-entropy loss assumes logits are **unnormalized** scores and automatically applies `log_softmax`.\n",
        "\n",
        "---\n",
        "\n",
        "These utilities allow flexible and efficient evaluation of your model on both full datasets and selected subsets.\n"
      ],
      "metadata": {
        "id": "rmoOAhl4lPJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes cross-entropy loss for a single batch\n",
        "def calc_loss_batch(input_batch,target_batch,model,device):\n",
        "  input_batch,target_batch=input_batch.to(device),target_batch.to(device)\n",
        "\n",
        "  logits=model(input_batch)         # Perform forward pass to get predicted logits | Shape: (batch_size, seq_len, vocab_size)\n",
        "\n",
        "  logits = torch.nan_to_num(logits, nan=0.0, posinf=0.0, neginf=0.0)#####\n",
        "  # Flatten predictions and targets for loss calculation\n",
        "  loss=torch.nn.functional.cross_entropy(\n",
        "      logits.flatten(0,1),          # Reshape to (batch_size * seq_len, vocab_size)\n",
        "      target_batch.flatten()        # Reshape to (batch_size * seq_len)\n",
        "  )\n",
        "  return loss\n",
        "\n",
        "\n",
        "# Computes average loss across multiple batches from a DataLoader\n",
        "def calc_loss_loader(data_loader,model,device,num_batches=None):\n",
        "  total_loss=0.\n",
        "\n",
        "  # Handle empty DataLoader\n",
        "  if len(data_loader)==0:\n",
        "    return float('nan')\n",
        "  elif num_batches is None:     # Determine number of batches to evaluate\n",
        "    num_batches=len(data_loader)\n",
        "  else:\n",
        "    num_batches=min(num_batches,len(data_loader))\n",
        "\n",
        "\n",
        "  # Iterate over the DataLoader and compute batch-wise loss\n",
        "  for i,(input_batch,target_batch) in enumerate(data_loader):\n",
        "    if i<num_batches:\n",
        "      loss=calc_loss_batch(input_batch,target_batch,model,device)\n",
        "      total_loss+=loss.item()\n",
        "    else:\n",
        "      break\n",
        "  return total_loss/num_batches         # Return mean loss over the evaluated batches"
      ],
      "metadata": {
        "id": "1x9T8zk5uJq5"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  train_loss=calc_loss_loader(train_loader,model,device)\n",
        "  val_loss=calc_loss_loader(val_loader,model,device)\n",
        "\n",
        "print('training loss: ',train_loss)\n",
        "print('validation loss: ',val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMPVHbTDxyil",
        "outputId": "bafcc40a-c1ad-4f8c-fd75-56af18fd6756"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss:  10.985039710998535\n",
            "validation loss:  10.943317413330078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation on Training and Validation Sets\n",
        "\n",
        "This function evaluates the model's performance by computing the **average cross-entropy loss** over a fixed number of batches from both the training and validation DataLoaders.\n",
        "\n",
        "---\n",
        "\n",
        "#### Function: `evaluate_model(model, train_loader, val_loader, device, eval_iter)`\n",
        "\n",
        "**Purpose**:  \n",
        "To periodically assess the model's performance during training by calculating the mean loss on:\n",
        "\n",
        "- A subset of the training data\n",
        "- A subset of the validation data\n",
        "\n",
        "---\n",
        "\n",
        "#### Key Steps:\n",
        "\n",
        "1. **Evaluation Mode**:\n",
        "   - `model.eval()` sets the model to evaluation mode, which:\n",
        "     - Disables dropout layers\n",
        "     - Uses running statistics in normalization layers (e.g., LayerNorm)\n",
        "\n",
        "2. **Disable Gradient Tracking**:\n",
        "   - `with torch.no_grad()` ensures no gradients are computed, saving memory and speeding up inference.\n",
        "\n",
        "3. **Compute Loss**:\n",
        "   - `calc_loss_loader(...)` is called separately on the training and validation loaders.\n",
        "   - Only the first `eval_iter` batches are evaluated (useful for fast and frequent evaluations).\n",
        "\n",
        "4. **Return to Training Mode**:\n",
        "   - `model.train()` restores training behavior (e.g., enables dropout) after evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "#### Returns:\n",
        "\n",
        "- `train_loss`: Mean loss over `eval_iter` training batches\n",
        "- `val_loss`: Mean loss over `eval_iter` validation batches\n",
        "\n",
        "---\n",
        "\n",
        "#### Use Case:\n",
        "\n",
        "- Can be called at regular intervals (e.g., every few epochs or steps) to monitor:\n",
        "  - Model convergence on training data\n",
        "  - Generalization to unseen validation data\n",
        "\n",
        "This enables you to detect **overfitting** or **underfitting** early during training.\n"
      ],
      "metadata": {
        "id": "9QkzJxd4lbnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluates the model on both training and validation data over a fixed number of batches\n",
        "def evaluate_model(model,train_loader,val_loader,device,eval_iter):\n",
        "  model.eval()            # Set the model to evaluation mode (disables dropout, uses running stats in norms)\n",
        "\n",
        "  # Disable gradient computation for efficient evaluation\n",
        "  with torch.no_grad():\n",
        "    train_loss=calc_loss_loader(train_loader,model,device,num_batches=eval_iter)      # Compute average loss over a subset of training data\n",
        "    val_loss=calc_loss_loader(val_loader,model,device,num_batches=eval_iter)          # Compute average loss over a subset of validation data\n",
        "\n",
        "  model.train()   # Return model to training mode after evaluation\n",
        "  return train_loss,val_loss"
      ],
      "metadata": {
        "id": "Z4535wPGzBrw"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text‚ÄìToken Conversion Utilities\n",
        "\n",
        "These helper functions convert between **raw text** and **token ID tensors** for GPT-style language models using the GPT-2 tokenizer.\n",
        "\n",
        "---\n",
        "\n",
        "#### `text_to_token_ids(text, tokenizer)`\n",
        "\n",
        "**Purpose**:  \n",
        "Converts a raw input string into a **token ID tensor** with an added batch dimension.\n",
        "\n",
        "**Steps**:\n",
        "- Tokenize the input text using the specified tokenizer (e.g., `tiktoken` for GPT-2).\n",
        "- `allowed_special={'<|endoftext|>'}` ensures that special tokens like `<|endoftext|>` are correctly recognized.\n",
        "- Convert the list of token IDs to a PyTorch tensor.\n",
        "- Add a **batch dimension** via `unsqueeze(0)` ‚Üí shape becomes `(1, seq_len)`.\n",
        "\n",
        "**Use Case**:\n",
        "Used for preparing raw text inputs to be passed into the model during generation or evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "#### `token_ids_to_text(token_ids, tokenizer)`\n",
        "\n",
        "**Purpose**:  \n",
        "Converts a token ID tensor back into a human-readable text string.\n",
        "\n",
        "**Steps**:\n",
        "- Assumes the input is a batch of token sequences.\n",
        "- Removes the batch dimension using `squeeze(0)`.\n",
        "- Converts the token ID list to a string using the tokenizer‚Äôs `decode()` method.\n",
        "\n",
        "**Use Case**:\n",
        "Used to convert model outputs (token predictions) back into readable text.\n",
        "\n",
        "---\n",
        "\n",
        "These functions help in the preprocessing and postprocessing pipeline of a text generation system.\n"
      ],
      "metadata": {
        "id": "FPE5f7sllm2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converts input text to a token ID tensor with batch dimension\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})       # Tokenize input text\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)                       # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "# Converts a batch of token IDs back to text (assumes batch size = 1)\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())"
      ],
      "metadata": {
        "id": "ad7BazTxAMt3"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt-Based Text Generation and Display\n",
        "\n",
        "This function takes a **text prompt**, feeds it to a trained GPT model, and prints the **generated continuation**. It wraps the full pipeline: tokenization ‚Üí generation ‚Üí decoding.\n",
        "\n",
        "---\n",
        "\n",
        "#### Function: `generate_and_print_sample(model, tokenizer, device, start_context)`\n",
        "\n",
        "**Purpose**:  \n",
        "To demonstrate text generation by producing and printing a continuation from a user-provided input prompt.\n",
        "\n",
        "---\n",
        "\n",
        "#### Workflow:\n",
        "\n",
        "1. **Model Evaluation Mode**:\n",
        "   - `model.eval()` disables dropout and other training-time behaviors to ensure consistent generation.\n",
        "\n",
        "2. **Determine Context Length**:\n",
        "   - Extract `context_size` from the positional embedding table: `model.pos_emb.weight.shape[0]`.\n",
        "\n",
        "3. **Tokenize Input**:\n",
        "   - Convert the `start_context` (prompt string) to token IDs with a batch dimension.\n",
        "   - Move the tensor to the appropriate device (CPU/GPU).\n",
        "\n",
        "4. **Generate Text**:\n",
        "   - Use `generate_text_simple(...)` to predict up to 50 new tokens in a greedy, autoregressive fashion.\n",
        "\n",
        "5. **Decode Output**:\n",
        "   - Convert the resulting token IDs back to human-readable text using the tokenizer.\n",
        "\n",
        "6. **Clean & Print**:\n",
        "   - Replace newline characters for cleaner output formatting.\n",
        "   - Print the complete generated text.\n",
        "\n",
        "7. **Return to Training Mode**:\n",
        "   - `model.train()` ensures the model continues training-ready after sample generation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3tVDW0Ncl0OU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates a continuation from a text prompt using the model and prints the decoded output\n",
        "def generate_and_print_sample(model,tokenizer,device,start_context):\n",
        "  model.eval()\n",
        "  context_size=model.pos_emb.weight.shape[0]                        # Determine the maximum context length from the positional embedding matrix\n",
        "  encoded=text_to_token_ids(start_context,tokenizer).to(device)     # Tokenize the input prompt and move to the specified device\n",
        "  with torch.no_grad():\n",
        "    # Generate new tokens starting from the input prompt\n",
        "    token_ids=generate_text_simple(\n",
        "        model=model,\n",
        "        idx=encoded,\n",
        "        max_new_tokens=50,\n",
        "        context_size=context_size\n",
        "    )\n",
        "\n",
        "    # Convert generated token IDs back to text\n",
        "    decoded_text=token_ids_to_text(token_ids,tokenizer)\n",
        "    print(decoded_text.replace('\\n',' '))\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "cw-FWijo-xog"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Training Loop for GPT Model\n",
        "\n",
        "This function defines a minimal training loop for a GPT-style model using a standard approach: forward pass ‚Üí loss computation ‚Üí backward pass ‚Üí optimizer step. It also includes periodic evaluation and sample generation.\n",
        "\n",
        "---\n",
        "\n",
        "#### Function: `train_model_simple(...)`\n",
        "\n",
        "**Inputs:**\n",
        "\n",
        "- `model`: The GPT model to train.\n",
        "- `train_loader`: DataLoader for training data.\n",
        "- `val_loader`: DataLoader for validation data.\n",
        "- `optimizer`: Optimizer (e.g., AdamW) for updating model weights.\n",
        "- `device`: `'cuda'` or `'cpu'`, depending on training hardware.\n",
        "- `num_epochs`: Total number of epochs to train.\n",
        "- `eval_freq`: Evaluate model after this many steps.\n",
        "- `eval_iter`: Number of batches used for evaluation.\n",
        "- `start_context`: Initial text prompt for generating a sample.\n",
        "- `tokenizer`: Tokenizer for converting between text and token IDs.\n",
        "\n",
        "---\n",
        "\n",
        "#### Training Loop Breakdown:\n",
        "\n",
        "1. **Initialize Tracking Lists**:\n",
        "   - `train_losses`, `val_losses`: Stores average loss values at each evaluation step.\n",
        "   - `track_tokens_seen`: Tracks how many tokens have been processed.\n",
        "\n",
        "2. **Outer Loop (Epochs)**:\n",
        "   - Loops through training data multiple times.\n",
        "\n",
        "3. **Inner Loop (Batches)**:\n",
        "   - For each batch:\n",
        "     - Zero the optimizer's gradient buffers.\n",
        "     - Compute the loss using `calc_loss_batch()`.\n",
        "     - Backpropagate and update weights.\n",
        "\n",
        "4. **Token Tracking**:\n",
        "   - `tokens_seen += input_batch.numel()` counts the number of tokens processed (for x-axis plotting or tracking training progress).\n",
        "\n",
        "5. **Periodic Evaluation**:\n",
        "   - Every `eval_freq` steps:\n",
        "     - Evaluate average training and validation loss using `evaluate_model()`.\n",
        "     - Store the results for later plotting.\n",
        "     - Print progress summary.\n",
        "\n",
        "6. **Text Generation**:\n",
        "   - After each epoch, generate a text sample from the current model using `generate_and_print_sample()`.\n",
        "\n",
        "7. **Final Output**:\n",
        "   - Returns:\n",
        "     - `train_losses`: List of training losses over time\n",
        "     - `val_losses`: List of validation losses over time\n",
        "     - `track_tokens_seen`: List of how many tokens were seen at each evaluation\n",
        "\n"
      ],
      "metadata": {
        "id": "9AJFIpEamCKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model,train_loader,val_loader,optimizer,device,num_epochs,eval_freq,eval_iter,start_context,tokenizer):\n",
        "  train_losses,val_losses,track_tokens_seen=[],[],[]\n",
        "  tokens_seen,global_step=0,-1\n",
        "\n",
        "  for i in range(num_epochs):\n",
        "    model.train() #set model to training mode\n",
        "    for input_batch,target_batch in train_loader:\n",
        "      optimizer.zero_grad() #reset loss gradient from previous batch iteration\n",
        "      loss=calc_loss_batch(input_batch,target_batch,model,device)\n",
        "      loss.backward() #calculate loss gradient\n",
        "      optimizer.step() #update model weights using loss gradients\n",
        "\n",
        "      # Increment tokens_seen before checking the eval_freq condition\n",
        "      tokens_seen += input_batch.numel() # total num of tokens\n",
        "      global_step+=1\n",
        "\n",
        "      if global_step%eval_freq==0:\n",
        "        train_loss,val_loss=evaluate_model(model,train_loader,val_loader,device,eval_iter)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        track_tokens_seen.append(tokens_seen)\n",
        "        print(f'Epoch {i+1} (Step {global_step:06d}): Train loss: {train_loss:.3f} val loss: {val_loss:.3f}')\n",
        "\n",
        "\n",
        "    generate_and_print_sample(model,tokenizer,device,start_context)\n",
        "  # Added return statement to fix the error. The function should return the accumulated losses and the number of tokens seen.\n",
        "  return train_losses, val_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "BC9VOE2W_6EK"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training GPT Model on Custom Dataset (124M Config)\n",
        "\n",
        "This script trains a GPT-style language model using a simplified training loop. The model is configured with 124 million parameters and trained on tokenized text data with periodic evaluation and sample generation.\n",
        "\n",
        "---\n",
        "\n",
        "#### Configuration Summary\n",
        "\n",
        "- **Model Size**: 124M parameters\n",
        "- **Context Length**: 1024 tokens\n",
        "- **Embedding Dim**: 768\n",
        "- **Layers / Heads**: 12 layers, 12 heads\n",
        "- **Dropout**: 0.1\n",
        "- **Batch Size**: 2\n",
        "- **Learning Rate**: 0.0004\n",
        "- **Weight Decay**: 0.1\n",
        "- **Epochs**: 10\n",
        "- **Evaluation Frequency**: every 5 steps\n",
        "\n",
        "---\n",
        "\n",
        "#### Results Snapshot\n",
        "\n",
        "- **Initial Losses**:  \n",
        "  - Epoch 1: Train loss = 9.72, Val loss = 10.16  \n",
        "  - Quickly drops to ~3.13 train loss by Epoch 7\n",
        "\n",
        "- **Best Generated Samples**:\n",
        "  - Epoch 2: `Every effort moves you the the, and the the, the, and, the the the.`\n",
        "  - Epoch 5: `Every effort moves you know, I had been, I had been to me--as...`\n",
        "  - Epoch 10: `Every effort moves you?\" I turned back to my work, and in fact...`\n",
        "\n",
        "- **Final Validation Loss**: ~6.34  \n",
        "- **Total Training Time**: ~0.62 minutes\n",
        "\n",
        "---\n",
        "\n",
        "#### Highlights\n",
        "\n",
        "- Model rapidly learns basic syntax and repetition.\n",
        "- Gradual semantic improvement across epochs.\n",
        "- Sample generation reflects typical autoregressive learning patterns (repetition ‚Üí coherence).\n"
      ],
      "metadata": {
        "id": "3bOjqaLDuGKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time=time.time()\n",
        "\n",
        "model=GPTModel(GPT_CONFIG_124m)\n",
        "model.to(device)\n",
        "optimizer=torch.optim.AdamW(model.parameters(),lr=0.0004,weight_decay=0.1)\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "num_epochs=10\n",
        "train_losses,val_losses,token_seen=train_model_simple(model,train_loader,val_loader,optimizer,device,num_epochs=num_epochs,eval_freq=5,eval_iter=5,start_context='Every effort moves you',tokenizer=tokenizer)\n",
        "end_time=time.time()\n",
        "print(f'Total time taken: {(end_time-start_time)/60:.2f} in Minutes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3WcRIYqFJYf",
        "outputId": "d0642c4a-76c7-4645-84a0-da5cd2bb155c"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (Step 000000): Train loss: 9.722 val loss: 10.162\n",
            "Epoch 1 (Step 000005): Train loss: 8.050 val loss: 8.338\n",
            "Every effort moves you                                                  \n",
            "Epoch 2 (Step 000010): Train loss: 6.757 val loss: 7.124\n",
            "Epoch 2 (Step 000015): Train loss: 6.043 val loss: 6.610\n",
            "Every effort moves you the the, and the the, the, and, the the the.                                   \n",
            "Epoch 3 (Step 000020): Train loss: 5.782 val loss: 6.497\n",
            "Epoch 3 (Step 000025): Train loss: 5.656 val loss: 6.512\n",
            "Every effort moves you\"I, and I, and his his his, and his the, and, and his I, and the I had the of the, and I had the, and, and I, and I, the, and of the, and,\n",
            "Epoch 4 (Step 000030): Train loss: 5.615 val loss: 6.561\n",
            "Epoch 4 (Step 000035): Train loss: 5.119 val loss: 6.444\n",
            "Every effort moves you I had, and I had that, and I had I had the had the had that he had had the had been I had the had the had had the had that he had the had the had had the had the had the had the had the\n",
            "Epoch 5 (Step 000040): Train loss: 4.784 val loss: 6.389\n",
            "Every effort moves you know, I had been, I had been to me--as, I had been to me--as, I had been to the picture.      \". \"--as the, I had been, I had been\n",
            "Epoch 6 (Step 000045): Train loss: 4.100 val loss: 6.282\n",
            "Epoch 6 (Step 000050): Train loss: 3.865 val loss: 6.243\n",
            "Every effort moves you know through, and in the picture.                                          \n",
            "Epoch 7 (Step 000055): Train loss: 3.608 val loss: 6.193\n",
            "Epoch 7 (Step 000060): Train loss: 3.134 val loss: 6.159\n",
            "Every effort moves you?\"  \"I didn't--I didn't--as, and in the house, and in the picture, I had to see, I had been his pictures--his.     \"I turned, and he was his\n",
            "Epoch 8 (Step 000065): Train loss: 2.587 val loss: 6.246\n",
            "Epoch 8 (Step 000070): Train loss: 2.276 val loss: 6.228\n",
            "Every effort moves you?\"            \"I turned back to my work, and in the fact, and in the house, and I felt to have him--I had longed to say: \"Be dissatisfied with your\n",
            "Epoch 9 (Step 000075): Train loss: 2.010 val loss: 6.241\n",
            "Epoch 9 (Step 000080): Train loss: 1.484 val loss: 6.260\n",
            "Every effort moves you?\"                                                 \n",
            "Epoch 10 (Step 000085): Train loss: 1.100 val loss: 6.341\n",
            "Every effort moves you?\"            \"I turned back to my work, and in fact, I had been taken.                    \n",
            "Total time taken: 0.62 in Minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced Text Generation with Temperature and Top-k Sampling\n",
        "\n",
        "This function generates tokens from a trained GPT model using advanced decoding strategies like temperature scaling, top-k sampling, and early stopping with an end-of-sequence token.\n",
        "\n",
        "---\n",
        "\n",
        "#### Function: `generate(...)`\n",
        "\n",
        "**Inputs:**\n",
        "\n",
        "- `model`: Trained GPT model.\n",
        "- `idx`: Input token IDs (shape: `(batch_size, sequence_length)`).\n",
        "- `max_new_tokens`: Maximum number of new tokens to generate.\n",
        "- `context_size`: Maximum number of recent tokens the model should consider.\n",
        "- `temperature`: Controls randomness. Higher = more diverse output. `0.0` = greedy.\n",
        "- `top_k`: If set, only the top-k most probable tokens are considered for sampling.\n",
        "- `eos_id`: Optional end-of-sequence token ID to stop generation early.\n",
        "\n",
        "---\n",
        "\n",
        "#### Decoding Breakdown:\n",
        "\n",
        "1. **Context Truncation**  \n",
        "   Only the last `context_size` tokens are used as input to the model for each generation step.\n",
        "\n",
        "2. **Logit Prediction**  \n",
        "   The model predicts the next-token logits. Only the logits for the last position are used.\n",
        "\n",
        "3. **Top-k Filtering (Optional)**  \n",
        "   If `top_k` is set, filter logits to retain only the top-k tokens and mask the rest with `-inf`.\n",
        "\n",
        "4. **Temperature Scaling (Optional)**  \n",
        "   If `temperature > 0`, divide logits by temperature before applying softmax and sample from the resulting distribution.\n",
        "\n",
        "5. **Greedy Decoding (Default)**  \n",
        "   If `temperature == 0`, use `argmax` to select the most likely token.\n",
        "\n",
        "6. **Early Stopping (Optional)**  \n",
        "   If `eos_id` is set and the generated token matches it, stop generation early.\n",
        "\n",
        "7. **Token Appending**  \n",
        "   Concatenate the new token to the running input sequence and repeat.\n"
      ],
      "metadata": {
        "id": "43Ok8odGt7wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Advanced decoding with temperature and top-k sampling\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "yonaQ7OwuG9N"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids('every effort moves you', tokenizer).to(device),\n",
        "    max_new_tokens=15,\n",
        "    context_size=GPT_CONFIG_124m['context_length'],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "print(token_ids_to_text(token_ids,tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLUdlU0nMShP",
        "outputId": "a0755cdc-9fab-4ae9-e271-46947dbd9c69"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "every effort moves you was \" not his pictures-- was a thought with a dep for--'s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving Model and Optimizer Checkpoint\n",
        "\n",
        "After training, it's important to save both the model's parameters and the optimizer's state to resume training or perform evaluation later.\n"
      ],
      "metadata": {
        "id": "Qj9ansTauX0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "torch.save({\n",
        "    'model_state_dict':model.state_dict(),\n",
        "    'optimizer_state_dict':optimizer.state_dict(),\n",
        "    },\n",
        "    'model_and_optimizer.pth')"
      ],
      "metadata": {
        "id": "0KFftBjQNlsb"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Loading Model and Optimizer from Checkpoint\n",
        "\n",
        "Use this snippet to restore a previously saved GPT model and its optimizer state from a `.pth` checkpoint file.\n"
      ],
      "metadata": {
        "id": "PtEWxBNDuf8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load checkpoint\n",
        "checkpoint=torch.load('/content/model_and_optimizer.pth')\n",
        "\n",
        "# Recreate model and load weights\n",
        "model=GPTModel(GPT_CONFIG_124m)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Recreate optimizer and load state\n",
        "optimizer=torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# Set model to training mode\n",
        "model.train();"
      ],
      "metadata": {
        "id": "7x8kNV8DQVnl"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOADING PRETRAINED WEIGHTS FROM OPENAI\n"
      ],
      "metadata": {
        "id": "UR7hmta5yvha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download3 import download_and_load_gpt2"
      ],
      "metadata": {
        "id": "0UQNKI8SRHkM"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw9EoS5by1W7",
        "outputId": "27e75cf4-8196-4370-da28-c1b5029a0e8f"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "checkpoint: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77.0/77.0 [00:00<00:00, 178kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "encoder.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.04M/1.04M [00:01<00:00, 572kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "hparams.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90.0/90.0 [00:00<00:00, 144kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.data-00000-of-00001: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 498M/498M [02:36<00:00, 3.18MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.index: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.21k/5.21k [00:00<00:00, 9.35MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.meta: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 471k/471k [00:01<00:00, 387kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "vocab.bpe: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 456k/456k [00:01<00:00, 373kiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dynamic GPT Model Configuration Setup\n",
        "\n",
        "This snippet enables flexible model creation by defining various GPT model sizes using a centralized configuration dictionary.\n",
        "\n",
        "---\n",
        "\n",
        "#### Purpose\n",
        "\n",
        "To easily switch between different GPT model sizes (small to XL) by:\n",
        "- Keeping a base configuration (`GPT_CONFIG_124m`)\n",
        "- Updating it dynamically based on the chosen model name"
      ],
      "metadata": {
        "id": "Ky6TRfpnvAhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model configurations in a dictionary for compactness\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "# Copy the base configuration and update with specific model settings\n",
        "model_name = \"gpt2-small (124M)\"  # Example model name\n",
        "NEW_CONFIG = GPT_CONFIG_124m.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n"
      ],
      "metadata": {
        "id": "dBIxWKnBy1T3"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
        "# Instantiate model using the updated configuration\n",
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval();"
      ],
      "metadata": {
        "id": "DEg8YUNry1QQ"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Safe Weight Assignment Helper\n",
        "\n",
        "This utility function is designed to **safely assign pretrained weights** to a PyTorch model parameter by checking shape compatibility before the assignment.\n",
        "\n",
        "---\n",
        "\n",
        "#### Function: `assign(left, right)`\n",
        "\n",
        "**Inputs:**\n",
        "- `left`: Target model parameter (e.g., from your current model)\n",
        "- `right`: Pretrained weight tensor you want to assign\n",
        "\n",
        "**Behavior:**\n",
        "1. Compares the shape of `left` and `right`.\n",
        "2. Raises a `ValueError` if shapes do not match.\n",
        "3. If shapes match, wraps the `right` tensor in `torch.nn.Parameter` and returns it.\n",
        "\n",
        "---\n",
        "\n",
        "#### Use Case\n",
        "\n",
        "This is especially useful during:\n",
        "- Manual weight loading from pretrained checkpoints\n",
        "- Model surgery or custom initialization\n",
        "- Debugging pretrained model mismatches"
      ],
      "metadata": {
        "id": "C78enaCYvLaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Safely assign pretrained weights to a model parameter, checking shape compatibility\n",
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))         # Wrap into a trainable torch parameter"
      ],
      "metadata": {
        "id": "zWW1LmJrznJr"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Pretrained Weights into Custom GPT Model\n",
        "\n",
        "This function populates a custom GPT architecture with pretrained weights from a dictionary (`params`) that follows the structure of Hugging Face GPT-style models.\n",
        "\n",
        "---\n",
        "\n",
        "#### Function: `load_weights_into_gpt(gpt, params)`\n",
        "\n",
        "**Inputs:**\n",
        "- `gpt`: Your custom GPT model instance.\n",
        "- `params`: A dictionary containing pretrained weights (e.g., from OpenAI or HuggingFace GPT-2).\n",
        "\n",
        "---\n",
        "\n",
        "#### Components Loaded:\n",
        "\n",
        "1. **Embeddings**\n",
        "   - `wpe`: Positional embeddings ‚Üí `gpt.pos_emb.weight`\n",
        "   - `wte`: Token embeddings ‚Üí `gpt.tok_emb.weight` and `gpt.out_head.weight`\n",
        "\n",
        "2. **Transformer Blocks (looped over `b`):**\n",
        "   - **QKV Weights & Biases**:\n",
        "     - Extracted from `attn.c_attn` and split into query/key/value\n",
        "     - Assigned to `W_query`, `W_key`, and `W_value`\n",
        "   - **Output Projection**:\n",
        "     - `attn.c_proj` ‚Üí `block.att.out_proj`\n",
        "   - **Feed-Forward Layers**:\n",
        "     - `mlp.c_fc` ‚Üí first linear layer\n",
        "     - `mlp.c_proj` ‚Üí final linear layer\n",
        "   - **Layer Norms**:\n",
        "     - `ln_1.g`, `ln_1.b` ‚Üí `block.norm1`\n",
        "     - `ln_2.g`, `ln_2.b` ‚Üí `block.norm2`\n",
        "\n",
        "3. **Final Layers**\n",
        "   - Final layer norm (`g`, `b`) ‚Üí `gpt.final_norm`\n",
        "   - Output head (`wte` reused) ‚Üí `gpt.out_head.weight`\n",
        "\n",
        "---\n",
        "\n",
        "#### Notes:\n",
        "- Uses the helper `assign()` to ensure shape compatibility before assigning weights.\n",
        "- Transposes weight matrices as required (`w.T`) to match PyTorch‚Äôs layout.\n"
      ],
      "metadata": {
        "id": "bUPZx23CveZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load pretrained weights from 'params' dictionary into your custom GPT model\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    # Assign positional and token embeddings\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])      # positional embeddings\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])      # token embeddings\n",
        "\n",
        "    # Iterate over the modules within the nn.Sequential trf_block\n",
        "    for b, block in enumerate(gpt.trf_block):\n",
        "        # Split QKV weights and assign\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        block.att.W_query.weight = assign(\n",
        "            block.att.W_query.weight, q_w.T)\n",
        "        block.att.W_key.weight = assign(\n",
        "            block.att.W_key.weight, k_w.T)\n",
        "        block.att.W_value.weight = assign(\n",
        "            block.att.W_value.weight, v_w.T)\n",
        "\n",
        "        # Split QKV biases and assign\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        block.att.W_query.bias = assign(\n",
        "            block.att.W_query.bias, q_b)\n",
        "        block.att.W_key.bias = assign(\n",
        "            block.att.W_key.bias, k_b)\n",
        "        block.att.W_value.bias = assign(\n",
        "            block.att.W_value.bias, v_b)\n",
        "\n",
        "        # Attention output projection\n",
        "        block.att.out_proj.weight = assign(\n",
        "            block.att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        block.att.out_proj.bias = assign(\n",
        "            block.att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        # Feed-forward network weights and biases\n",
        "        block.ff.layers[0].weight = assign(\n",
        "            block.ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        block.ff.layers[0].bias = assign(\n",
        "            block.ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        block.ff.layers[2].weight = assign(\n",
        "            block.ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        block.ff.layers[2].bias = assign(\n",
        "            block.ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        # Layer normalization parameters for attention and MLP sublayers\n",
        "        block.norm1.scale = assign(\n",
        "            block.norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        block.norm1.shift = assign(\n",
        "            block.norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        block.norm2.scale = assign(\n",
        "            block.norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        block.norm2.shift = assign(\n",
        "            block.norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    # Final layer normalization and output projection\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ],
      "metadata": {
        "id": "nKtSHFnfzquN"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device);"
      ],
      "metadata": {
        "id": "xLQ_Jt4IzulL"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
        "    max_new_tokens=25,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1.4\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGZhNeplz7iQ",
        "outputId": "8b4c10ec-2641-4880-ed42-e8dba4487506"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you as far as the hand can go until the end of your turn. By now there's a very small delay before you gain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINETUNING FOR CLASSIFICATION\n"
      ],
      "metadata": {
        "id": "EENBn0eQQ2oW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and Prepare the SMS Spam Collection Dataset\n",
        "\n",
        "This script downloads the SMS Spam Collection dataset from the UCI repository, unzips it, and renames the main data file with a `.tsv` extension for easier use.\n"
      ],
      "metadata": {
        "id": "fbmV12-vwBIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import ssl\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
        "    if data_file_path.exists():\n",
        "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
        "        return\n",
        "\n",
        "    # Create an unverified SSL context\n",
        "    ssl_context = ssl._create_unverified_context()\n",
        "\n",
        "    # Downloading the file\n",
        "    with urllib.request.urlopen(url, context=ssl_context) as response:\n",
        "        with open(zip_path, \"wb\") as out_file:\n",
        "            out_file.write(response.read())\n",
        "\n",
        "    # Unzipping the file\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extracted_path)\n",
        "\n",
        "    # Add .tsv file extension\n",
        "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
        "    os.rename(original_file_path, data_file_path)\n",
        "    print(f\"File downloaded and saved as {data_file_path}\")\n",
        "\n",
        "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smrik7qKQ4n7",
        "outputId": "4272078f-e7e1-41b6-af4e-7fb578518996"
      },
      "execution_count": 428,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "oilv6XiZQ9-6",
        "outputId": "5b69c1a3-c144-4e2d-de0c-bf898079ff10"
      },
      "execution_count": 429,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Label                                               Text\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
              "...    ...                                                ...\n",
              "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
              "5568   ham               Will √º b going to esplanade fr home?\n",
              "5569   ham  Pity, * was in mood for that. So...any other s...\n",
              "5570   ham  The guy did some bitching but I acted like i'd...\n",
              "5571   ham                         Rofl. Its true to its name\n",
              "\n",
              "[5572 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-35a2c245-d531-4787-8cb1-dfadcc03b5c4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will √º b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows √ó 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35a2c245-d531-4787-8cb1-dfadcc03b5c4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-35a2c245-d531-4787-8cb1-dfadcc03b5c4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-35a2c245-d531-4787-8cb1-dfadcc03b5c4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e146e428-a371-42d3-8892-f6c963f74d89\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e146e428-a371-42d3-8892-f6c963f74d89')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e146e428-a371-42d3-8892-f6c963f74d89 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_4211ad20-db1c-41f4-a0a8-151ff70c7908\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4211ad20-db1c-41f4-a0a8-151ff70c7908 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5572,\n  \"fields\": [\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5169,\n        \"samples\": [\n          \"K, makes sense, btw carlos is being difficult so you guys are gonna smoke while I go pick up the second batch and get gas\",\n          \"URGENT! Your mobile No *********** WON a \\u00a32,000 Bonus Caller Prize on 02/06/03! This is the 2nd attempt to reach YOU! Call 09066362220 ASAP! BOX97N7QP, 150ppm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 429
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"Label\"].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4ElfQPjRBrU",
        "outputId": "87d65747-b0b6-455a-f25a-0cd061896c5e"
      },
      "execution_count": 430,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Balanced Dataset (Spam vs. Ham)\n",
        "\n",
        "This function balances the dataset by **undersampling** the majority class (\"ham\") to match the number of instances in the minority class (\"spam\"). Useful when dealing with imbalanced binary classification problems like spam detection."
      ],
      "metadata": {
        "id": "Yimezi7L6yZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_balanced_dataset(df):\n",
        "    # Count the instances of \"spam\"\n",
        "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
        "\n",
        "    # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
        "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
        "\n",
        "    # Combine ham \"subset\" with \"spam\"\n",
        "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "balanced_df = create_balanced_dataset(df)\n",
        "print(balanced_df[\"Label\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOSnYsNAREfA",
        "outputId": "83bfa2a2-e55f-490c-8adf-f9dbfd1ae17b"
      },
      "execution_count": 431,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     747\n",
            "spam    747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert Text Labels to Numeric Values\n",
        "\n",
        "To prepare the labels for model training, map `\"ham\"` to `0` and `\"spam\"` to `1`:\n"
      ],
      "metadata": {
        "id": "4j708qRu7BM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
      ],
      "metadata": {
        "id": "QKy2GIpGRGv8"
      },
      "execution_count": 432,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-Validation-Test Split Function\n",
        "\n",
        "This function shuffles a DataFrame and splits it into **train**, **validation**, and **test**"
      ],
      "metadata": {
        "id": "Ev28eFZP66Yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_split(df, train_frac, validation_frac):\n",
        "    # Shuffle the entire DataFrame\n",
        "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "\n",
        "    # Calculate split indices\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "    # Split the DataFrame\n",
        "    train_df = df[:train_end]\n",
        "    validation_df = df[train_end:validation_end]\n",
        "    test_df = df[validation_end:]\n",
        "\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
        "# Test size is implied to be 0.2 as the remainder\n"
      ],
      "metadata": {
        "id": "sCZgM-W7RI-g"
      },
      "execution_count": 433,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_df))\n",
        "print(len(validation_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olA9xjlaRKwC",
        "outputId": "b6f723e6-6d90-44d0-8ac7-29e3100d6a84"
      },
      "execution_count": 434,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1045\n",
            "149\n",
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)"
      ],
      "metadata": {
        "id": "9oNNMQ9eRNMP"
      },
      "execution_count": 435,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom PyTorch Dataset: `SpamDataset`\n",
        "\n",
        "This class creates a PyTorch-compatible dataset for SMS spam classification using pre-tokenized inputs.\n",
        "\n",
        "---\n",
        "\n",
        "#### Class: `SpamDataset`\n",
        "\n",
        "**Inputs:**\n",
        "- `csv_file`: Path to the CSV file containing the dataset.\n",
        "- `tokenizer`: Tokenizer to convert text into token IDs.\n",
        "- `max_length` (optional): Maximum sequence length. If not provided, uses the longest sequence in the dataset.\n",
        "- `pad_token_id` (default=50256): Token ID used for padding sequences to `max_length`.\n",
        "\n",
        "---\n",
        "\n",
        "#### Main Features:\n",
        "\n",
        "1. **Tokenization**:\n",
        "   - Tokenizes each message in the `\"Text\"` column using the provided `tokenizer`.\n",
        "\n",
        "2. **Truncation and Padding**:\n",
        "   - Truncates any sequence longer than `max_length`.\n",
        "   - Pads all sequences to `max_length` using `pad_token_id`.\n",
        "\n",
        "3. **Label Handling**:\n",
        "   - Converts each sample‚Äôs label from the `\"Label\"` column into a PyTorch tensor.\n",
        "\n",
        "4. **Dataset Interface**:\n",
        "   - `__getitem__(index)`: Returns `(input_tensor, label_tensor)` for the given index.\n",
        "   - `__len__()`: Returns total number of samples.\n",
        "   - `_longest_encoded_length()`: Calculates length of the longest encoded text for dynamic padding.\n",
        "\n"
      ],
      "metadata": {
        "id": "mG-Z-U_d7JrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = [\n",
        "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
        "        ]\n",
        "\n",
        "        if max_length is None:\n",
        "            self.max_length = self._longest_encoded_length()\n",
        "        else:\n",
        "            self.max_length = max_length\n",
        "            # Truncate sequences if they are longer than max_length\n",
        "            self.encoded_texts = [\n",
        "                encoded_text[:self.max_length]\n",
        "                for encoded_text in self.encoded_texts\n",
        "            ]\n",
        "\n",
        "        # Pad sequences to the longest sequence\n",
        "        self.encoded_texts = [\n",
        "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
        "            for encoded_text in self.encoded_texts\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoded = self.encoded_texts[index]\n",
        "        label = self.data.iloc[index][\"Label\"]\n",
        "        return (\n",
        "            torch.tensor(encoded, dtype=torch.long),\n",
        "            torch.tensor(label, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _longest_encoded_length(self):\n",
        "        max_length = 0\n",
        "        for encoded_text in self.encoded_texts:\n",
        "            encoded_length = len(encoded_text)\n",
        "            if encoded_length > max_length:\n",
        "                max_length = encoded_length\n",
        "        return max_length"
      ],
      "metadata": {
        "id": "UGGZFEtARPik"
      },
      "execution_count": 436,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SpamDataset(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=None,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(train_dataset.max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMMIUiniTkzT",
        "outputId": "01329651-68a7-43a6-c829-a5bb672b53bd"
      },
      "execution_count": 437,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = SpamDataset(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "test_dataset = SpamDataset(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "ftM0QE8CTnIW"
      },
      "execution_count": 438,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")"
      ],
      "metadata": {
        "id": "VcfK-hJBUD6w"
      },
      "execution_count": 439,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Configuration & Validation\n",
        "\n",
        "This block sets up the configuration for a GPT model variant (e.g., GPT-2 Small) and ensures that the input dataset does not exceed the model‚Äôs maximum context length.\n",
        "\n",
        "---\n",
        "\n",
        "#### Key Elements\n",
        "\n",
        "- **`CHOOSE_MODEL`**: Specifies which GPT-2 variant to use (e.g., `gpt2-small (124M)`).\n",
        "- **`INPUT_PROMPT`**: Initial text prompt used for generation.\n",
        "\n",
        "---\n",
        "\n",
        "#### `BASE_CONFIG` Dictionary\n",
        "\n",
        "Defines the base model configuration:\n",
        "\n",
        "- `vocab_size`: Size of the tokenizer vocabulary.\n",
        "- `context_length`: Maximum number of tokens the model can attend to.\n",
        "- `dropout`: Dropout rate for training regularization.\n",
        "- `qkv_bias`: Whether to apply a bias term to the query/key/value projections in attention.\n",
        "\n",
        "---\n",
        "\n",
        "#### `model_configs` Dictionary\n",
        "\n",
        "Maps each GPT-2 model variant to its architectural specs:\n",
        "\n",
        "- `emb_dim`: Embedding dimension.\n",
        "- `n_layers`: Number of transformer layers.\n",
        "- `n_heads`: Number of attention heads.\n",
        "\n",
        "The chosen model‚Äôs configuration is merged into `BASE_CONFIG`.\n",
        "\n"
      ],
      "metadata": {
        "id": "M2u3-at67cPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
        "INPUT_PROMPT = \"Every effort moves\"\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"dropout\": 0.0,          # Dropout rate #drop_rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
        "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
        "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
        "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
        ")"
      ],
      "metadata": {
        "id": "uM4niHuRVAoC"
      },
      "execution_count": 440,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = \"Every effort moves you\"\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(text_1, tokenizer),\n",
        "    max_new_tokens=15,\n",
        "    context_size=BASE_CONFIG[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTU5X27-sP1T",
        "outputId": "c8c7a409-ef50-40cf-8000-db8613b44556"
      },
      "execution_count": 441,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Every effort moves you know.\n",
            "\"--, the picture the the, you know the the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_2 = (\n",
        "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
        "    \" 'You are a winner you have been specially\"\n",
        "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
        ")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(text_2, tokenizer),\n",
        "    max_new_tokens=23,\n",
        "    context_size=BASE_CONFIG[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVe9PH20yWE-",
        "outputId": "7c19db55-62df-4294-bb06-2a30a1b32a20"
      },
      "execution_count": 442,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.', and he had been was, you know a good- the picture was his pictures, the picture was a of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT Model Architecture Overview (GPT-2 Small - 124M)\n",
        "\n",
        "The following is a summary of the architecture for the GPT-style transformer model based on the 124M parameter configuration:\n",
        "\n",
        "---\n",
        "\n",
        "#### Model Components:\n",
        "\n",
        "- **Embedding Layers:**\n",
        "  - `tok_emb`: Token embedding of shape `(vocab_size=50257, emb_dim=768)`\n",
        "  - `pos_emb`: Positional embedding of shape `(context_length=1024, emb_dim=768)`\n",
        "  - `drop_emb`: Dropout applied to the embeddings (p=0.1)\n",
        "\n",
        "- **Transformer Stack (`trf_block`):**\n",
        "  - `12` stacked `TransformerBlock` modules, each containing:\n",
        "    - **MultiHeadAttention**:\n",
        "      - `W_query`, `W_key`, `W_value`: Linear layers (768 ‚Üí 768), **no bias**\n",
        "      - `out_proj`: Output projection (768 ‚Üí 768), **with bias**\n",
        "      - `dropout`: Applied to attention output (p=0.1)\n",
        "    - **FeedForward** (MLP):\n",
        "      - Sequential layers: `Linear(768 ‚Üí 3072) ‚Üí GELU ‚Üí Linear(3072 ‚Üí 768)`\n",
        "    - **Layer Normalizations:**\n",
        "      - `norm1` (pre-attention), `norm2` (pre-MLP)\n",
        "    - **Residual Dropout**: `drop_shortcut` (p=0.1)\n",
        "\n",
        "- **Final Layers:**\n",
        "  - `final_norm`: Layer normalization before output\n",
        "  - `out_head`: Final linear layer mapping hidden states to vocabulary logits `(768 ‚Üí 50257)`\n",
        "\n",
        "---\n",
        "\n",
        "#### Notes:\n",
        "\n",
        "- **Dropout** is consistently used to regularize both embeddings and residual paths.\n",
        "- **No bias** is used in attention projections (as controlled by `qkv_bias=False`).\n",
        "- **Autoregressive model**: Only uses causal self-attention.\n",
        "- **Compatible with pretrained GPT-2 configurations** (via appropriate weight mapping).\n",
        "\n",
        "This configuration corresponds to **GPT-2 Small (124M parameters)** with:\n",
        "- 12 layers (transformer blocks)\n",
        "- 12 attention heads\n",
        "- Hidden size of 768\n"
      ],
      "metadata": {
        "id": "L1rkwmMzHgmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhOzaCX2ye8z",
        "outputId": "0791a209-4ec6-43af-9bb7-523336033103"
      },
      "execution_count": 443,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPTModel(\n",
            "  (tok_emb): Embedding(50257, 768)\n",
            "  (pos_emb): Embedding(1024, 768)\n",
            "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
            "  (trf_block): Sequential(\n",
            "    (0): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (2): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (3): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (4): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (5): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (6): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (7): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (8): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (9): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (10): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (11): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_norm): LayerNorm()\n",
            "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning GPT for Classification (Freeze Except Last Block)\n",
        "\n",
        "This setup freezes the majority of a GPT model's layers and fine-tunes only the last transformer block, final normalization, and replaces the output head for classification (e.g., spam vs ham).\n"
      ],
      "metadata": {
        "id": "sMwqkdn-IKSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all GPT model parameters\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "4H9aCw773uiL"
      },
      "execution_count": 444,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "# Replace the output head with a classification head (e.g., 2 classes)\n",
        "num_classes = 2\n",
        "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
      ],
      "metadata": {
        "id": "eMQI1G4r44p6"
      },
      "execution_count": 445,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze only the final transformer block\n",
        "for param in model.trf_block[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze the final layer normalization layer\n",
        "for param in model.final_norm.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "sIFBWuzQ46T4"
      },
      "execution_count": 446,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Single Text Classification Inference\n",
        "We pass a prompt through the model and obtain logits for the last token, which are then softmaxed to get class probabilities."
      ],
      "metadata": {
        "id": "OPLt5kxqJcOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode(\"Do you have time\")\n",
        "inputs = torch.tensor(inputs).unsqueeze(0)\n",
        "print(\"Inputs:\", inputs)\n",
        "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs)\n",
        "\n",
        "print(\"Outputs:\\n\", outputs)\n",
        "print(\"Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)\n",
        "\n",
        "print(\"Last output token:\", outputs[:, -1, :])\n",
        "\n",
        "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
        "label = torch.argmax(probas)\n",
        "print(\"Class label:\", label.item())\n",
        "\n",
        "logits = outputs[:, -1, :]\n",
        "label = torch.argmax(logits)\n",
        "print(\"Class label:\", label.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sul7jJUX5iBu",
        "outputId": "31a84192-5c04-44d8-d84c-b00db321c77f"
      },
      "execution_count": 447,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: tensor([[5211,  345,  423,  640]])\n",
            "Inputs dimensions: torch.Size([1, 4])\n",
            "Outputs:\n",
            " tensor([[[-0.3217, -0.1943],\n",
            "         [ 0.1498, -0.6487],\n",
            "         [ 0.0912, -0.1845],\n",
            "         [ 0.2911, -1.1208]]])\n",
            "Outputs dimensions: torch.Size([1, 4, 2])\n",
            "Last output token: tensor([[ 0.2911, -1.1208]])\n",
            "Class label: 0\n",
            "Class label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy Evaluation Function for GPT Classifier\n",
        "\n",
        "This utility calculates the token-level accuracy using the final output token of GPT for classification tasks (e.g., spam vs. ham).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-VM3c_myIbm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculates token-level accuracy over a data loader\n",
        "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
        "    model.eval()\n",
        "    correct_predictions, num_examples = 0, 0\n",
        "\n",
        "    # Determine number of batches to evaluate\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    # Iterate through the batches\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "            # Disable gradient computation for inference\n",
        "            with torch.no_grad():\n",
        "                logits = model(input_batch)[:, -1, :]   # Logits of last output token\n",
        "            predicted_labels = torch.argmax(logits, dim=-1)   # Get predicted class indices\n",
        "\n",
        "            num_examples += predicted_labels.shape[0]   # Update number of evaluated samples\n",
        "\n",
        "            # NOTE: Ensure this compares to the correct target token\n",
        "            # If target_batch is full sequence, select only the last token\n",
        "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
        "        else:\n",
        "            break\n",
        "    # print(\"correct: \",correct_predictions)\n",
        "    # print(\"total: \",num_examples)\n",
        "    return correct_predictions / num_examples"
      ],
      "metadata": {
        "id": "XT2-6tZm9ykw"
      },
      "execution_count": 448,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Evaluation on Subset of Dataset\n",
        "We evaluate the model's performance on a small sample from each dataset (10 batches) for quick feedback on fine-tuning effectiveness.\n",
        "\n",
        "| Dataset    | Accuracy (%) |\n",
        "| ---------- | ------------ |\n",
        "| Training   | 53.75%       |\n",
        "| Validation | 55.00%       |\n",
        "| Test       | 51.25%       |\n",
        "\n",
        "\n",
        "**Note:** These results are based on 10 batches per set and may reflect underfitting. Consider training longer, adjusting learning rate, or unfreezing more layers for improved performance.\n"
      ],
      "metadata": {
        "id": "sGUt2s04Jnxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the training data loader\n",
        "\n",
        "# Evaluate classification accuracy on 10 batches from each dataset\n",
        "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3sYdPyQ-G6Z",
        "outputId": "32a80ec1-67f7-4b0b-95c4-237274f64533"
      },
      "execution_count": 449,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 53.75%\n",
            "Validation accuracy: 55.00%\n",
            "Test accuracy: 51.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy Loss Calculation for GPT Classifier\n",
        "\n",
        "This function computes the **cross-entropy loss** between the predicted logits of the **last token** and the ground truth labels. It is tailored for classification tasks using a GPT-style model where only the final token output is used for prediction.\n",
        "\n",
        "\n",
        "\n",
        "#### Usage:\n",
        "- `input_batch`: Tensor of token IDs, shape `(batch_size, sequence_length)`\n",
        "- `target_batch`: Corresponding ground truth class labels\n",
        "- `model`: GPT model adapted for classification\n",
        "- `device`: `cuda` or `cpu`\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** This approach assumes only the final token is used for classification (typical for GPT fine-tuning on tasks like spam detection).\n"
      ],
      "metadata": {
        "id": "UkyLrZVCKTw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute cross-entropy loss for the last token prediction in a batch\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    # Forward pass through the model and select logits of the last token\n",
        "    logits = model(input_batch)[:, -1, :]         # Logits of last output token    Shape: (batch_size, vocab_size)\n",
        "    # Compute cross-entropy loss between predicted logits and target token indices\n",
        "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "almaMOsN-UXg"
      },
      "execution_count": 450,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average Cross-Entropy Loss over DataLoader\n",
        "\n",
        "This utility function computes the **average cross-entropy loss** across multiple batches from a PyTorch `DataLoader`. It is designed for evaluating GPT-style classifiers that use only the **final token's logits** for classification.\n",
        "\n",
        "---\n",
        "\n",
        "#### Summary:\n",
        "- **Purpose:** Evaluate model loss on a subset (or all) of a dataset.\n",
        "- **Granularity:** Operates on **batch-level** using the `calc_loss_batch` function.\n",
        "- **Safe:** Handles empty datasets and respects batch limits.\n",
        "\n",
        "Ideal for **validation** or **testing** loops during fine-tuning of transformer-based classifiers.\n"
      ],
      "metadata": {
        "id": "wbzeveXtKgFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculates the average cross-entropy loss over multiple batches from a DataLoader\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.                                                                 # Initialize running total of loss\n",
        "\n",
        "    # Edge case: if the DataLoader is empty, return NaN\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    # Determine how many batches to evaluate (either all or a limited number)\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))                            # Cap num_batches at the size of the DataLoader to avoid index overflow\n",
        "\n",
        "    # Loop over the DataLoader and accumulate loss\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)        # Compute loss for this batch using the provided loss function\n",
        "            total_loss += loss.item()                                               # Accumulate the scalar loss value\n",
        "        else:\n",
        "            break                                                                   # Stop early if max number of batches is reached\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "PKq-30YCCWFP"
      },
      "execution_count": 451,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Evaluation (Subset of Dataset)\n",
        "\n",
        "We compute the **cross-entropy loss** on a subset of each dataset (5 batches) to quickly gauge model performance during evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "####  Results:\n",
        "\n",
        "| Dataset    | Cross-Entropy Loss |\n",
        "|------------|--------------------|\n",
        "| Training   | **0.772**          |\n",
        "| Validation | **0.764**          |\n",
        "| Test       | **0.789**          |\n",
        "\n",
        ">  *Note:* Loss values are averaged over only 5 batches from each split and may not represent the full dataset performance. Fine-tuning or more batch coverage may improve the results.\n",
        "\n"
      ],
      "metadata": {
        "id": "vgGtRlQTKrfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
        "\n",
        "print(f\"Training loss: {train_loss:.3f}\")\n",
        "print(f\"Validation loss: {val_loss:.3f}\")\n",
        "print(f\"Test loss: {test_loss:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLg5EEMgCYg8",
        "outputId": "2cc058f5-2b67-4f6a-bd76-3045bc2f1c3d"
      },
      "execution_count": 452,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.772\n",
            "Validation loss: 0.764\n",
            "Test loss: 0.789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINETUNING THE MODEL ON SUPERVISED DATA"
      ],
      "metadata": {
        "id": "vmaPWXPlCgNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Training Loop for GPT-based Text Classifier\n",
        "\n",
        "This function trains a GPT-style model for text classification using **cross-entropy loss** and **accuracy evaluation** on both training and validation datasets.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### Highlights:\n",
        "- Evaluates loss every `eval_freq` steps on `eval_iter` batches.\n",
        "- Tracks both **loss** and **accuracy** across epochs.\n",
        "- Returns all key metrics for visualization or logging:\n",
        "  - Training & validation loss\n",
        "  - Training & validation accuracy\n",
        "  - Total examples seen\n",
        "\n",
        "> Use this for efficient fine-tuning or transfer learning on classification tasks like spam detection.\n"
      ],
      "metadata": {
        "id": "Dz1qxDRCK3AJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Overall the same as `train_model_simple` in chapter 5\n",
        "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                            eval_freq, eval_iter):\n",
        "    # Initialize lists to track losses and examples seen\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    examples_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            examples_seen += input_batch.shape[0] # New: track examples instead of tokens\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Calculate accuracy after each epoch\n",
        "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "        train_accs.append(train_accuracy)\n",
        "        val_accs.append(val_accuracy)\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
      ],
      "metadata": {
        "id": "lL9sDZgZCanm"
      },
      "execution_count": 453,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Function for GPT Classifier\n",
        "\n",
        "This utility function computes the **average loss** on both training and validation sets using a fixed number of batches. It is typically called during training to monitor progress.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### Purpose:\n",
        "- Returns **average cross-entropy loss** over `eval_iter` batches for:\n",
        "  - `train_loader`\n",
        "  - `val_loader`\n",
        "\n",
        "#### Usage:\n",
        "- Designed to be called inside a training loop, typically at intervals like `every N steps`.\n",
        "- Supports quick feedback without evaluating on the full dataset.\n",
        "\n",
        "> Efficient way to monitor overfitting and convergence during training.\n"
      ],
      "metadata": {
        "id": "-pIwIdaELBVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluates model loss on training and validation data over a fixed number of batches\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "id": "IoknSVEACieH"
      },
      "execution_count": 454,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT Classifier Training Summary\n",
        "\n",
        "This section documents the training of a GPT-style model (fine-tuned for binary classification) using the **AdamW optimizer**, over **5 epochs** with periodic evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "#### Training Configuration:\n",
        "\n",
        "- **Epochs:** `5`\n",
        "- **Optimizer:** `AdamW`\n",
        "- **Learning Rate:** `5e-5`\n",
        "- **Weight Decay:** `0.1`\n",
        "- **Evaluation Frequency:** every `50` steps\n",
        "- **Evaluation Batches per Step:** `5`\n",
        "- **Seed:** `123`\n",
        "- **Total Time:** `~1.06 minutes`\n",
        "\n",
        "---\n",
        "\n",
        "#### Training & Validation Loss (Selected Steps)\n",
        "\n",
        "| Step   | Train Loss | Val Loss |\n",
        "|--------|------------|----------|\n",
        "| 0      | 0.899      | 0.847    |\n",
        "| 50     | 0.385      | 0.441    |\n",
        "| 100    | 0.260      | 0.377    |\n",
        "| 150    | 0.447      | 0.344    |\n",
        "| 200    | 0.334      | 0.323    |\n",
        "| 250    | 0.401      | 0.296    |\n",
        "| 300    | 0.236      | 0.296    |\n",
        "| 350    | 0.208      | 0.209    |\n",
        "| 400    | 0.108      | 0.171    |\n",
        "| 450    | 0.099      | 0.170    |\n",
        "| 500    | 0.182      | 0.256    |\n",
        "| 550    | 0.169      | 0.147    |\n",
        "| 600    | 0.120      | 0.151    |\n",
        "\n",
        "---\n",
        "\n",
        "#### Epoch-wise Accuracy:\n",
        "\n",
        "| Epoch | Training Accuracy | Validation Accuracy |\n",
        "|-------|-------------------|---------------------|\n",
        "| 1     | 85.00%            | 82.50%              |\n",
        "| 2     | 82.50%            | 82.50%              |\n",
        "| 3     | 87.50%            | 87.50%              |\n",
        "| 4     | 95.00%            | 90.00%              |\n",
        "| 5     | 100.00%           | 95.00%              |\n",
        "\n",
        "---\n",
        "\n",
        "#### Observations:\n",
        "\n",
        "- The model shows **progressive improvement** in both loss and accuracy.\n",
        "- Final validation accuracy reached **95%**, suggesting effective fine-tuning.\n",
        "- Potential signs of slight overfitting in later epochs (train acc = 100%).\n",
        "\n",
        "> Consider early stopping or regularization if performance starts degrading.\n",
        "\n"
      ],
      "metadata": {
        "id": "UHRpyzkGLPr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "num_epochs = 5\n",
        "\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1Iq2P3QCkKS",
        "outputId": "21143c75-0894-4473-aff2-9522ec7da188"
      },
      "execution_count": 455,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 0.899, Val loss 0.847\n",
            "Ep 1 (Step 000050): Train loss 0.385, Val loss 0.441\n",
            "Ep 1 (Step 000100): Train loss 0.260, Val loss 0.377\n",
            "Training accuracy: 85.00% | Validation accuracy: 82.50%\n",
            "Ep 2 (Step 000150): Train loss 0.447, Val loss 0.344\n",
            "Ep 2 (Step 000200): Train loss 0.334, Val loss 0.323\n",
            "Ep 2 (Step 000250): Train loss 0.401, Val loss 0.296\n",
            "Training accuracy: 82.50% | Validation accuracy: 82.50%\n",
            "Ep 3 (Step 000300): Train loss 0.236, Val loss 0.296\n",
            "Ep 3 (Step 000350): Train loss 0.208, Val loss 0.209\n",
            "Training accuracy: 87.50% | Validation accuracy: 87.50%\n",
            "Ep 4 (Step 000400): Train loss 0.108, Val loss 0.171\n",
            "Ep 4 (Step 000450): Train loss 0.099, Val loss 0.170\n",
            "Ep 4 (Step 000500): Train loss 0.182, Val loss 0.256\n",
            "Training accuracy: 95.00% | Validation accuracy: 90.00%\n",
            "Ep 5 (Step 000550): Train loss 0.169, Val loss 0.147\n",
            "Ep 5 (Step 000600): Train loss 0.120, Val loss 0.151\n",
            "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
            "Training completed in 1.06 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting Training & Validation Metrics\n",
        "\n",
        "This function visualizes the progression of **loss or accuracy** over training epochs. It supports dual x-axes: one for epochs and one for the number of examples seen.\n",
        "\n",
        "#### Features:\n",
        "- Plots both training and validation curves\n",
        "- Shows **epochs** on the bottom x-axis and **examples seen** on the top\n",
        "- Automatically saves the plot as a PDF\n",
        "\n",
        "\n",
        "> Call this function with `label=\"loss\"` or `label=\"accuracy\"` to produce the respective plots.\n"
      ],
      "metadata": {
        "id": "zropdJjXLbY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
        "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(label.capitalize())\n",
        "    ax1.legend()\n",
        "\n",
        "    # Create a second x-axis for examples seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Examples seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(f\"{label}-plot.pdf\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qPWBwDbnCmHT"
      },
      "execution_count": 456,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training vs. Validation Loss Curve\n",
        "\n",
        "The plot below illustrates how the training and validation loss evolve over time during the fine-tuning of the GPT-2 Small classifier on the SMS Spam dataset.\n",
        "\n",
        "- **X-axis (bottom):** Epochs\n",
        "- **X-axis (top):** Number of examples seen\n",
        "- **Y-axis:** Cross-entropy loss\n",
        "\n",
        "| Metric         | Observation                           |\n",
        "|----------------|----------------------------------------|\n",
        "| Training Loss  | Decreases steadily, shows convergence |\n",
        "| Validation Loss| Follows training loss, no overfitting detected |\n",
        "\n",
        "\n",
        "> ‚úÖ **Interpretation:** The model is learning effectively with both training and validation losses decreasing consistently. Validation loss aligns well with training loss, indicating generalization without overfitting.\n"
      ],
      "metadata": {
        "id": "DD2b3ESMLrHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
        "\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "jMhdT0-PC10K",
        "outputId": "d56292bb-a020-4702-b725-9161db59c73e"
      },
      "execution_count": 457,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training vs. Validation Accuracy Curve\n",
        "\n",
        "This plot visualizes how the classification accuracy of the GPT-2 Small model improves over training epochs on the SMS Spam detection task.\n",
        "\n",
        "- **X-axis (bottom):** Epochs\n",
        "- **X-axis (top):** Examples seen during training\n",
        "- **Y-axis:** Accuracy (0 to 1.0)\n",
        "\n",
        "| Metric           | Observation                                |\n",
        "|------------------|---------------------------------------------|\n",
        "| Training Accuracy | Increases steadily to 100% by final epoch |\n",
        "| Validation Accuracy | Closely tracks training, reaches ~95%    |\n",
        "\n",
        "\n",
        "> **Interpretation:** The model shows strong learning progress without signs of overfitting. The small gap between training and validation accuracy suggests effective generalization.\n"
      ],
      "metadata": {
        "id": "wF2WeAxHL6pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
        "\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "Hf6TXPsNC4Ux",
        "outputId": "2f163830-cc3a-4946-d031-ba94cd52ec3a"
      },
      "execution_count": 458,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Accuracy Evaluation (Full Dataset)\n",
        "\n",
        "After completing fine-tuning of the GPT-2 Small classifier on the SMS Spam dataset, the model was evaluated on the **entire** training, validation, and test sets.\n",
        "\n",
        "| Dataset    | Accuracy (%) |\n",
        "|------------|--------------|\n",
        "| Training   | **98.75%**   |\n",
        "| Validation | **95.97%**   |\n",
        "| Test       | **94.00%**   |\n",
        "\n",
        "> **Conclusion:**  \n",
        "The model demonstrates **strong generalization** across all sets. High training accuracy (98.75%) paired with competitive validation and test accuracies suggests that the classifier has **successfully learned to distinguish spam from ham messages** without overfitting.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4VGJxZ2TMFa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RruPxTsOC7fG",
        "outputId": "053ef2e0-dfa3-4b51-a102-f1a6d64f5ee8"
      },
      "execution_count": 459,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 98.75%\n",
            "Validation accuracy: 95.97%\n",
            "Test accuracy: 94.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "USING THE LLM AS A SPAM CLASSIFIER"
      ],
      "metadata": {
        "id": "LoB4CctKDCuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SMS Spam Classification Utility\n",
        "\n",
        "The `classify_review` function performs **inference** using a fine-tuned GPT model to classify a single text message as `\"spam\"` or `\"not spam\"`.\n",
        "\n",
        "#### Function Breakdown:\n",
        "\n",
        "| Step | Description |\n",
        "|------|-------------|\n",
        "| 1    | Tokenize input using GPT tokenizer |\n",
        "| 2    | Truncate to `max_length` or model‚Äôs context limit |\n",
        "| 3    | Pad input to ensure fixed length |\n",
        "| 4    | Add batch dimension (batch size = 1) |\n",
        "| 5    | Disable gradient computation for efficiency |\n",
        "| 6    | Extract final token‚Äôs logits for classification |\n",
        "| 7    | Return `\"spam\"` or `\"not spam\"` based on prediction |\n",
        "\n",
        "> Make sure to pass `max_length=BASE_CONFIG[\"context_length\"]` to prevent dimension mismatch.\n"
      ],
      "metadata": {
        "id": "vVj6EMREDFYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare inputs to the model\n",
        "    # Step 1: Prepare inputs to the model\n",
        "    input_ids = tokenizer.encode(text)\n",
        "    supported_context_length = model.pos_emb.weight.shape[0]\n",
        "    # Note: In the book, this was originally written as pos_emb.weight.shape[1] by mistake\n",
        "    # It didn't break the code but would have caused unnecessary truncation (to 768 instead of 1024)\n",
        "\n",
        "    # Step 2: Truncate sequences if they are too long\n",
        "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
        "\n",
        "    # Step 3: Pad sequences to match max_length\n",
        "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
        "\n",
        "    # Step 4: Add batch dimension\n",
        "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension\n",
        "\n",
        "    # Model inference\n",
        "    # Step 5: Model inference without gradient tracking\n",
        "    with torch.no_grad():\n",
        "        # Step 6: Use logits from the last output token\n",
        "        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token\n",
        "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    # Return the classified result\n",
        "    return \"spam\" if predicted_label == 1 else \"not spam\"       # Step 7: Return the classified result"
      ],
      "metadata": {
        "id": "v90fKMaCC-j5"
      },
      "execution_count": 460,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Classify a Spam SMS using GPT Classifier\n"
      ],
      "metadata": {
        "id": "Dk0rKlLPNKpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = (\n",
        "    \"You are a winner you have been specially\"\n",
        "    \" selected to receive $1000 cash or a $2000 award.\"\n",
        ")\n",
        "\n",
        "print(classify_review(\n",
        "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndmefMLWDHwk",
        "outputId": "529b6257-973a-466e-c2fb-97c000a6e4fe"
      },
      "execution_count": 461,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_2 = (\n",
        "    \"Hey, just wanted to check if we're still on\"\n",
        "    \" for dinner tonight? Let me know!\"\n",
        ")\n",
        "\n",
        "print(classify_review(\n",
        "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHtKA3TsDJeh",
        "outputId": "f492fb08-fb85-4f02-909d-9f0e6131d45d"
      },
      "execution_count": 462,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = (\n",
        "    \"Congratulations! You've been selected for a free cruise to the Bahamas. Call now to claim your prize.\"\n",
        ")\n",
        "print(classify_review(\n",
        "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))\n",
        "\n",
        "text_2 = (\n",
        "    \"URGENT! Your mobile number has won $5000. Click the link to claim: http://bit.ly/winprize\"\n",
        ")\n",
        "print(classify_review(\n",
        "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))\n",
        "\n",
        "text_3 = (\n",
        "    \"Limited time offer: Buy 1 get 2 FREE! Visit our store today or call 123-456-7890.\"\n",
        ")\n",
        "print(classify_review(\n",
        "    text_3, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jfiLljlDLI-",
        "outputId": "f58d013c-c490-401c-cb83-d20554f04d4b"
      },
      "execution_count": 467,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam\n",
            "spam\n",
            "spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_4 = (\n",
        "    \"Don‚Äôt forget our meeting tomorrow at 10 AM. Let me know if the time works for you.\"\n",
        ")\n",
        "print(classify_review(\n",
        "    text_4, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))\n",
        "\n",
        "text_5 = (\n",
        "    \"Just wanted to say happy birthday! Hope you have an amazing day ahead üéâ\"\n",
        ")\n",
        "print(classify_review(\n",
        "    text_5, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))\n",
        "\n",
        "text_6 = (\n",
        "    \"Can you send me the notes from today's lecture? I missed the class.\"\n",
        ")\n",
        "print(classify_review(\n",
        "    text_6, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcPXBMJDV8ux",
        "outputId": "38e4fd9d-1735-4cce-9bac-44e45d1f86ad"
      },
      "execution_count": 468,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not spam\n",
            "not spam\n",
            "not spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1t1W0Y7nWWpd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}